{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../flower_photos/'\n",
    "w = 100\n",
    "h = 100\n",
    "c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    cate=[path+x for x in os.listdir(path) if os.path.isdir(path+x)] \n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for idx, folder in enumerate(cate):\n",
    "        for im in glob.glob(folder + '/*.jpg'):\n",
    "            img = cv2.imread(im)\n",
    "            img = cv2.resize(img, (w, h))\n",
    "            imgs.append(img)\n",
    "            labels.append(idx)\n",
    "    return np.asarray(imgs, np.float32), np.asarray(labels, np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([739, 3, 100, 100]), torch.Size([739]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = read_img(path)\n",
    "\n",
    "data = torch.FloatTensor(data).permute(0, 3, 1, 2)\n",
    "label = torch.LongTensor(label)\n",
    "\n",
    "data.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 109\n",
    "np.random.seed(seed)\n",
    "\n",
    "(x_train, x_val, y_train, y_val) = train_test_split(data, label, test_size=0.20, random_state=seed)\n",
    "x_train = x_train / 255\n",
    "x_val = x_val / 255\n",
    "\n",
    "flower_dict = {0:'bee', 1:'blackberry', 2:'blanket', 3:'bougainvilliea', 4:'bromelia', 5:'foxglove'}\n",
    "class_list = ['bee', 'blackberry', 'blanket', 'bougainvilliea', 'bromelia', 'foxglove']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TORCH框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        super(FlowerDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    model\n",
    "    采取和tf一样的CNN架构\n",
    "'''\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.conv_model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(12*12*64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 6),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.conv_model(data)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stat:\n",
    "    def __init__(self, training, writer=None):\n",
    "        self.step = 0\n",
    "        self.loss = []\n",
    "        self.labels = []\n",
    "        self.pred_labels = []\n",
    "        self.training = training\n",
    "        self.writer = writer\n",
    "    \n",
    "    def add(self, pred, labels, loss):\n",
    "        labels = labels.numpy()\n",
    "        pred = pred.detach().numpy()\n",
    "        pred_labels = np.argmax(pred, axis = 1)\n",
    "        self.loss.append(loss)\n",
    "        self.labels.extend(labels)\n",
    "        self.pred_labels.extend(pred_labels)\n",
    "\n",
    "    def log(self):\n",
    "        self.step += 1\n",
    "        acc = accuracy_score(self.labels, self.pred_labels)\n",
    "        loss = sum(self.loss) / len(self.loss)\n",
    "        self.loss = []\n",
    "        self.labels = []\n",
    "        self.pred_labels = []\n",
    "        if not self.writer:\n",
    "            return loss, acc\n",
    "        if self.training:\n",
    "            self.writer.add_scalar('train_loss', loss, self.step)\n",
    "            self.writer.add_scalar('train_acc', acc, self.step)\n",
    "        else:\n",
    "            self.writer.add_scalar('dev_loss', loss, self.step)\n",
    "            self.writer.add_scalar('dev_acc', acc, self.step)\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, dev_data_loader):\n",
    "    loss_func = CrossEntropyLoss()\n",
    "    optimizer = Adam(\n",
    "        model.parameters(),\n",
    "        lr=0.001,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    writer = SummaryWriter('./summary/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n",
    "    train_stat = Stat(training=True, writer=writer)\n",
    "    dev_stat = Stat(training=False, writer=writer)\n",
    "\n",
    "\n",
    "    best_acc, best_net = 0.0, None\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"--- epoch: {epoch + 1} ---\")\n",
    "        for iter, batch in enumerate(train_data_loader):\n",
    "            model.train()\n",
    "            data, labels = batch[0], batch[1]\n",
    "            pred_outputs = model(data)\n",
    "            loss = loss_func(pred_outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_stat.add(pred_outputs, labels, loss.item())\n",
    "            \n",
    "        train_loss, train_acc = train_stat.log()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in dev_data_loader:\n",
    "                data, labels = batch[0], batch[1]\n",
    "                pred_outputs = model(data)\n",
    "                loss = loss_func(pred_outputs, labels)\n",
    "                dev_stat.add(pred_outputs, labels, loss.item())\n",
    "        dev_loss, dev_acc = dev_stat.log()\n",
    "        print(  f\"training loss: {train_loss:.4f}, acc: {train_acc:.2%}, \" \\\n",
    "                f\"dev loss: {dev_loss:.4f}, acc: {dev_acc:.2%}.\")\n",
    "\n",
    "        if dev_acc > best_acc:\n",
    "            best_acc = dev_acc\n",
    "            best_net = deepcopy(model.state_dict())\n",
    "            \n",
    "    print(f\"best dev acc: {best_acc:.4f}\")\n",
    "    return best_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'C:\\\\Users\\zzzgry\\Desktop\\midspore_lab2\\model\\\\pytorch\\\\model-'+time.strftime('%m-%d_%H.%M', time.localtime())+'.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FlowerDataset(x_train, y_train)\n",
    "dev_set = FlowerDataset(x_val, y_val)\n",
    "\n",
    "train_data_loader = DataLoader(train_set, 32, True)\n",
    "dev_data_loader = DataLoader(dev_set, 32, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 1 ---\n",
      "training loss: 1.7004, acc: 29.10%, dev loss: 1.5619, acc: 40.54%.\n",
      "--- epoch: 2 ---\n",
      "training loss: 1.5119, acc: 40.78%, dev loss: 1.3819, acc: 45.27%.\n",
      "--- epoch: 3 ---\n",
      "training loss: 1.3706, acc: 45.35%, dev loss: 1.3154, acc: 48.65%.\n",
      "--- epoch: 4 ---\n",
      "training loss: 1.2609, acc: 51.10%, dev loss: 1.1462, acc: 50.68%.\n",
      "--- epoch: 5 ---\n",
      "training loss: 1.1829, acc: 54.65%, dev loss: 1.1110, acc: 57.43%.\n",
      "--- epoch: 6 ---\n",
      "training loss: 1.1465, acc: 55.67%, dev loss: 1.2429, acc: 50.68%.\n",
      "--- epoch: 7 ---\n",
      "training loss: 1.0804, acc: 58.04%, dev loss: 0.9935, acc: 62.84%.\n",
      "--- epoch: 8 ---\n",
      "training loss: 1.0161, acc: 62.10%, dev loss: 1.0795, acc: 60.14%.\n",
      "--- epoch: 9 ---\n",
      "training loss: 0.9563, acc: 64.30%, dev loss: 0.9587, acc: 67.57%.\n",
      "--- epoch: 10 ---\n",
      "training loss: 0.9510, acc: 64.64%, dev loss: 0.9909, acc: 63.51%.\n",
      "--- epoch: 11 ---\n",
      "training loss: 0.8554, acc: 68.53%, dev loss: 0.8978, acc: 66.22%.\n",
      "--- epoch: 12 ---\n",
      "training loss: 0.8746, acc: 68.19%, dev loss: 0.9180, acc: 63.51%.\n",
      "--- epoch: 13 ---\n",
      "training loss: 0.8080, acc: 69.37%, dev loss: 0.9624, acc: 62.16%.\n",
      "--- epoch: 14 ---\n",
      "training loss: 0.8588, acc: 69.54%, dev loss: 1.0287, acc: 61.49%.\n",
      "--- epoch: 15 ---\n",
      "training loss: 0.8294, acc: 69.54%, dev loss: 0.9177, acc: 62.84%.\n",
      "--- epoch: 16 ---\n",
      "training loss: 0.7582, acc: 71.40%, dev loss: 0.8665, acc: 68.24%.\n",
      "--- epoch: 17 ---\n",
      "training loss: 0.7578, acc: 71.57%, dev loss: 0.8457, acc: 65.54%.\n",
      "--- epoch: 18 ---\n",
      "training loss: 0.6992, acc: 74.28%, dev loss: 0.9988, acc: 58.78%.\n",
      "--- epoch: 19 ---\n",
      "training loss: 0.8267, acc: 68.19%, dev loss: 0.9488, acc: 66.22%.\n",
      "--- epoch: 20 ---\n",
      "training loss: 0.6988, acc: 73.60%, dev loss: 0.8561, acc: 64.19%.\n",
      "--- epoch: 21 ---\n",
      "training loss: 0.6673, acc: 75.63%, dev loss: 0.8703, acc: 69.59%.\n",
      "--- epoch: 22 ---\n",
      "training loss: 0.5929, acc: 79.02%, dev loss: 0.8221, acc: 69.59%.\n",
      "--- epoch: 23 ---\n",
      "training loss: 0.6350, acc: 74.79%, dev loss: 0.9239, acc: 67.57%.\n",
      "--- epoch: 24 ---\n",
      "training loss: 0.6613, acc: 75.63%, dev loss: 0.9029, acc: 68.24%.\n",
      "--- epoch: 25 ---\n",
      "training loss: 0.6643, acc: 76.99%, dev loss: 0.8642, acc: 68.92%.\n",
      "--- epoch: 26 ---\n",
      "training loss: 0.6146, acc: 77.50%, dev loss: 0.8581, acc: 68.92%.\n",
      "--- epoch: 27 ---\n",
      "training loss: 0.5726, acc: 79.36%, dev loss: 0.8700, acc: 64.86%.\n",
      "--- epoch: 28 ---\n",
      "training loss: 0.5770, acc: 78.85%, dev loss: 0.8882, acc: 70.27%.\n",
      "--- epoch: 29 ---\n",
      "training loss: 0.5146, acc: 80.88%, dev loss: 0.8760, acc: 67.57%.\n",
      "--- epoch: 30 ---\n",
      "training loss: 0.5169, acc: 81.39%, dev loss: 0.9477, acc: 66.22%.\n",
      "--- epoch: 31 ---\n",
      "training loss: 0.4997, acc: 81.22%, dev loss: 0.9231, acc: 70.27%.\n",
      "--- epoch: 32 ---\n",
      "training loss: 0.5251, acc: 82.91%, dev loss: 0.8003, acc: 71.62%.\n",
      "--- epoch: 33 ---\n",
      "training loss: 0.5000, acc: 81.73%, dev loss: 0.8044, acc: 69.59%.\n",
      "--- epoch: 34 ---\n",
      "training loss: 0.4735, acc: 81.90%, dev loss: 0.9051, acc: 67.57%.\n",
      "--- epoch: 35 ---\n",
      "training loss: 0.5085, acc: 81.90%, dev loss: 0.8895, acc: 69.59%.\n",
      "--- epoch: 36 ---\n",
      "training loss: 0.4823, acc: 82.40%, dev loss: 0.9233, acc: 68.92%.\n",
      "--- epoch: 37 ---\n",
      "training loss: 0.4856, acc: 81.05%, dev loss: 1.0404, acc: 68.92%.\n",
      "--- epoch: 38 ---\n",
      "training loss: 0.4752, acc: 81.05%, dev loss: 0.8570, acc: 66.89%.\n",
      "--- epoch: 39 ---\n",
      "training loss: 0.4084, acc: 86.80%, dev loss: 0.9445, acc: 70.95%.\n",
      "--- epoch: 40 ---\n",
      "training loss: 0.4006, acc: 85.96%, dev loss: 0.9470, acc: 66.22%.\n",
      "--- epoch: 41 ---\n",
      "training loss: 0.4364, acc: 83.42%, dev loss: 0.9360, acc: 63.51%.\n",
      "--- epoch: 42 ---\n",
      "training loss: 0.4342, acc: 84.26%, dev loss: 0.8297, acc: 70.95%.\n",
      "--- epoch: 43 ---\n",
      "training loss: 0.4187, acc: 85.11%, dev loss: 0.9282, acc: 66.89%.\n",
      "--- epoch: 44 ---\n",
      "training loss: 0.4279, acc: 87.48%, dev loss: 0.9154, acc: 66.89%.\n",
      "--- epoch: 45 ---\n",
      "training loss: 0.3615, acc: 86.29%, dev loss: 0.8306, acc: 73.65%.\n",
      "--- epoch: 46 ---\n",
      "training loss: 0.3673, acc: 87.99%, dev loss: 0.9323, acc: 72.30%.\n",
      "--- epoch: 47 ---\n",
      "training loss: 0.3817, acc: 87.14%, dev loss: 0.9532, acc: 68.24%.\n",
      "--- epoch: 48 ---\n",
      "training loss: 0.3859, acc: 86.29%, dev loss: 0.9980, acc: 67.57%.\n",
      "--- epoch: 49 ---\n",
      "training loss: 0.3588, acc: 85.79%, dev loss: 0.8876, acc: 70.27%.\n",
      "--- epoch: 50 ---\n",
      "training loss: 0.3732, acc: 85.96%, dev loss: 0.8911, acc: 71.62%.\n",
      "--- epoch: 51 ---\n",
      "training loss: 0.3537, acc: 88.16%, dev loss: 0.9302, acc: 68.24%.\n",
      "--- epoch: 52 ---\n",
      "training loss: 0.2962, acc: 90.36%, dev loss: 0.9981, acc: 72.30%.\n",
      "--- epoch: 53 ---\n",
      "training loss: 0.2660, acc: 89.51%, dev loss: 1.0095, acc: 66.22%.\n",
      "--- epoch: 54 ---\n",
      "training loss: 0.2994, acc: 88.83%, dev loss: 0.9395, acc: 68.24%.\n",
      "--- epoch: 55 ---\n",
      "training loss: 0.3258, acc: 88.49%, dev loss: 0.9264, acc: 71.62%.\n",
      "--- epoch: 56 ---\n",
      "training loss: 0.3039, acc: 88.66%, dev loss: 0.9995, acc: 68.24%.\n",
      "--- epoch: 57 ---\n",
      "training loss: 0.3042, acc: 87.14%, dev loss: 0.9795, acc: 70.95%.\n",
      "--- epoch: 58 ---\n",
      "training loss: 0.2542, acc: 91.54%, dev loss: 0.9362, acc: 69.59%.\n",
      "--- epoch: 59 ---\n",
      "training loss: 0.2512, acc: 91.37%, dev loss: 1.3058, acc: 68.24%.\n",
      "--- epoch: 60 ---\n",
      "training loss: 0.3438, acc: 86.13%, dev loss: 0.8850, acc: 73.65%.\n",
      "--- epoch: 61 ---\n",
      "training loss: 0.2848, acc: 89.17%, dev loss: 0.9349, acc: 73.65%.\n",
      "--- epoch: 62 ---\n",
      "training loss: 0.2551, acc: 91.54%, dev loss: 1.0366, acc: 70.27%.\n",
      "--- epoch: 63 ---\n",
      "training loss: 0.2860, acc: 89.68%, dev loss: 1.0257, acc: 71.62%.\n",
      "--- epoch: 64 ---\n",
      "training loss: 0.2699, acc: 91.54%, dev loss: 0.9609, acc: 70.27%.\n",
      "--- epoch: 65 ---\n",
      "training loss: 0.3036, acc: 90.19%, dev loss: 0.9675, acc: 70.95%.\n",
      "--- epoch: 66 ---\n",
      "training loss: 0.3101, acc: 89.85%, dev loss: 1.0418, acc: 70.27%.\n",
      "--- epoch: 67 ---\n",
      "training loss: 0.2858, acc: 90.69%, dev loss: 0.9727, acc: 72.30%.\n",
      "--- epoch: 68 ---\n",
      "training loss: 0.2667, acc: 92.89%, dev loss: 1.0937, acc: 70.27%.\n",
      "--- epoch: 69 ---\n",
      "training loss: 0.2485, acc: 91.54%, dev loss: 0.9431, acc: 71.62%.\n",
      "--- epoch: 70 ---\n",
      "training loss: 0.2618, acc: 91.71%, dev loss: 0.9153, acc: 73.65%.\n",
      "--- epoch: 71 ---\n",
      "training loss: 0.2811, acc: 90.52%, dev loss: 0.9204, acc: 72.97%.\n",
      "--- epoch: 72 ---\n",
      "training loss: 0.3071, acc: 90.36%, dev loss: 0.9761, acc: 73.65%.\n",
      "--- epoch: 73 ---\n",
      "training loss: 0.2356, acc: 91.20%, dev loss: 0.9763, acc: 65.54%.\n",
      "--- epoch: 74 ---\n",
      "training loss: 0.2221, acc: 92.22%, dev loss: 0.9018, acc: 75.00%.\n",
      "--- epoch: 75 ---\n",
      "training loss: 0.2099, acc: 92.22%, dev loss: 1.0587, acc: 66.89%.\n",
      "--- epoch: 76 ---\n",
      "training loss: 0.2037, acc: 94.08%, dev loss: 0.9597, acc: 72.30%.\n",
      "--- epoch: 77 ---\n",
      "training loss: 0.2387, acc: 91.71%, dev loss: 0.9371, acc: 71.62%.\n",
      "--- epoch: 78 ---\n",
      "training loss: 0.2306, acc: 91.71%, dev loss: 1.1195, acc: 68.92%.\n",
      "--- epoch: 79 ---\n",
      "training loss: 0.2321, acc: 92.22%, dev loss: 0.9294, acc: 72.30%.\n",
      "--- epoch: 80 ---\n",
      "training loss: 0.2283, acc: 93.23%, dev loss: 1.1595, acc: 71.62%.\n",
      "--- epoch: 81 ---\n",
      "training loss: 0.2656, acc: 90.02%, dev loss: 0.8803, acc: 72.97%.\n",
      "--- epoch: 82 ---\n",
      "training loss: 0.2244, acc: 93.23%, dev loss: 0.9578, acc: 72.97%.\n",
      "--- epoch: 83 ---\n",
      "training loss: 0.2066, acc: 93.23%, dev loss: 0.8562, acc: 75.68%.\n",
      "--- epoch: 84 ---\n",
      "training loss: 0.1921, acc: 92.72%, dev loss: 0.9224, acc: 75.00%.\n",
      "--- epoch: 85 ---\n",
      "training loss: 0.2411, acc: 91.20%, dev loss: 0.9142, acc: 74.32%.\n",
      "--- epoch: 86 ---\n",
      "training loss: 0.2432, acc: 91.54%, dev loss: 0.8933, acc: 74.32%.\n",
      "--- epoch: 87 ---\n",
      "training loss: 0.1820, acc: 94.42%, dev loss: 0.9803, acc: 75.68%.\n",
      "--- epoch: 88 ---\n",
      "training loss: 0.1942, acc: 93.40%, dev loss: 0.9361, acc: 70.95%.\n",
      "--- epoch: 89 ---\n",
      "training loss: 0.1944, acc: 93.40%, dev loss: 1.0155, acc: 72.97%.\n",
      "--- epoch: 90 ---\n",
      "training loss: 0.2101, acc: 92.22%, dev loss: 0.9382, acc: 71.62%.\n",
      "--- epoch: 91 ---\n",
      "training loss: 0.1682, acc: 95.09%, dev loss: 0.8852, acc: 74.32%.\n",
      "--- epoch: 92 ---\n",
      "training loss: 0.2121, acc: 92.89%, dev loss: 0.9750, acc: 72.97%.\n",
      "--- epoch: 93 ---\n",
      "training loss: 0.1998, acc: 93.74%, dev loss: 0.9749, acc: 68.92%.\n",
      "--- epoch: 94 ---\n",
      "training loss: 0.1905, acc: 94.25%, dev loss: 1.1070, acc: 68.92%.\n",
      "--- epoch: 95 ---\n",
      "training loss: 0.2125, acc: 92.72%, dev loss: 0.9745, acc: 75.68%.\n",
      "--- epoch: 96 ---\n",
      "training loss: 0.2048, acc: 92.05%, dev loss: 0.9323, acc: 75.00%.\n",
      "--- epoch: 97 ---\n",
      "training loss: 0.2125, acc: 93.40%, dev loss: 1.1278, acc: 72.30%.\n",
      "--- epoch: 98 ---\n",
      "training loss: 0.1979, acc: 93.06%, dev loss: 0.8813, acc: 74.32%.\n",
      "--- epoch: 99 ---\n",
      "training loss: 0.1791, acc: 93.57%, dev loss: 0.9283, acc: 79.05%.\n",
      "--- epoch: 100 ---\n",
      "training loss: 0.1610, acc: 95.09%, dev loss: 0.9251, acc: 76.35%.\n",
      "--- epoch: 101 ---\n",
      "training loss: 0.1828, acc: 93.40%, dev loss: 0.9921, acc: 72.97%.\n",
      "--- epoch: 102 ---\n",
      "training loss: 0.1591, acc: 95.43%, dev loss: 0.9673, acc: 77.03%.\n",
      "--- epoch: 103 ---\n",
      "training loss: 0.1983, acc: 93.23%, dev loss: 0.8673, acc: 75.00%.\n",
      "--- epoch: 104 ---\n",
      "training loss: 0.1514, acc: 95.60%, dev loss: 0.9308, acc: 75.68%.\n",
      "--- epoch: 105 ---\n",
      "training loss: 0.1665, acc: 94.59%, dev loss: 0.8529, acc: 75.00%.\n",
      "--- epoch: 106 ---\n",
      "training loss: 0.2032, acc: 93.23%, dev loss: 0.8636, acc: 72.97%.\n",
      "--- epoch: 107 ---\n",
      "training loss: 0.2061, acc: 94.25%, dev loss: 0.9454, acc: 72.30%.\n",
      "--- epoch: 108 ---\n",
      "training loss: 0.1708, acc: 93.74%, dev loss: 0.8275, acc: 75.68%.\n",
      "--- epoch: 109 ---\n",
      "training loss: 0.1621, acc: 94.08%, dev loss: 1.0512, acc: 73.65%.\n",
      "--- epoch: 110 ---\n",
      "training loss: 0.1924, acc: 93.57%, dev loss: 0.9202, acc: 72.97%.\n",
      "--- epoch: 111 ---\n",
      "training loss: 0.1629, acc: 94.08%, dev loss: 0.8608, acc: 78.38%.\n",
      "--- epoch: 112 ---\n",
      "training loss: 0.1464, acc: 95.77%, dev loss: 0.9034, acc: 76.35%.\n",
      "--- epoch: 113 ---\n",
      "training loss: 0.1757, acc: 94.59%, dev loss: 0.8669, acc: 75.68%.\n",
      "--- epoch: 114 ---\n",
      "training loss: 0.1637, acc: 94.59%, dev loss: 0.9446, acc: 71.62%.\n",
      "--- epoch: 115 ---\n",
      "training loss: 0.1578, acc: 93.40%, dev loss: 0.8877, acc: 75.00%.\n",
      "--- epoch: 116 ---\n",
      "training loss: 0.2210, acc: 92.55%, dev loss: 0.9232, acc: 75.00%.\n",
      "--- epoch: 117 ---\n",
      "training loss: 0.1619, acc: 94.42%, dev loss: 0.9975, acc: 76.35%.\n",
      "--- epoch: 118 ---\n",
      "training loss: 0.1651, acc: 95.09%, dev loss: 0.7548, acc: 79.05%.\n",
      "--- epoch: 119 ---\n",
      "training loss: 0.1443, acc: 95.77%, dev loss: 0.9166, acc: 77.03%.\n",
      "--- epoch: 120 ---\n",
      "training loss: 0.1945, acc: 93.91%, dev loss: 0.7447, acc: 79.73%.\n",
      "--- epoch: 121 ---\n",
      "training loss: 0.2222, acc: 92.72%, dev loss: 0.8159, acc: 77.03%.\n",
      "--- epoch: 122 ---\n",
      "training loss: 0.1780, acc: 94.92%, dev loss: 0.7930, acc: 75.68%.\n",
      "--- epoch: 123 ---\n",
      "training loss: 0.1276, acc: 96.45%, dev loss: 0.9129, acc: 73.65%.\n",
      "--- epoch: 124 ---\n",
      "training loss: 0.1575, acc: 94.75%, dev loss: 0.9060, acc: 77.03%.\n",
      "--- epoch: 125 ---\n",
      "training loss: 0.1454, acc: 94.25%, dev loss: 0.9885, acc: 71.62%.\n",
      "--- epoch: 126 ---\n",
      "training loss: 0.1320, acc: 95.43%, dev loss: 0.8303, acc: 77.03%.\n",
      "--- epoch: 127 ---\n",
      "training loss: 0.1660, acc: 94.08%, dev loss: 0.8625, acc: 70.27%.\n",
      "--- epoch: 128 ---\n",
      "training loss: 0.1770, acc: 95.43%, dev loss: 0.7719, acc: 76.35%.\n",
      "--- epoch: 129 ---\n",
      "training loss: 0.1685, acc: 94.25%, dev loss: 0.8834, acc: 78.38%.\n",
      "--- epoch: 130 ---\n",
      "training loss: 0.1519, acc: 94.75%, dev loss: 1.0178, acc: 72.30%.\n",
      "--- epoch: 131 ---\n",
      "training loss: 0.1341, acc: 95.94%, dev loss: 1.1010, acc: 73.65%.\n",
      "--- epoch: 132 ---\n",
      "training loss: 0.1879, acc: 93.40%, dev loss: 0.8211, acc: 76.35%.\n",
      "--- epoch: 133 ---\n",
      "training loss: 0.1488, acc: 95.94%, dev loss: 0.8401, acc: 75.68%.\n",
      "--- epoch: 134 ---\n",
      "training loss: 0.1698, acc: 95.26%, dev loss: 0.8196, acc: 77.70%.\n",
      "--- epoch: 135 ---\n",
      "training loss: 0.1369, acc: 96.28%, dev loss: 0.8645, acc: 77.03%.\n",
      "--- epoch: 136 ---\n",
      "training loss: 0.1517, acc: 95.26%, dev loss: 1.1095, acc: 68.92%.\n",
      "--- epoch: 137 ---\n",
      "training loss: 0.1752, acc: 95.94%, dev loss: 0.8541, acc: 74.32%.\n",
      "--- epoch: 138 ---\n",
      "training loss: 0.1670, acc: 93.74%, dev loss: 0.8989, acc: 73.65%.\n",
      "--- epoch: 139 ---\n",
      "training loss: 0.1337, acc: 95.43%, dev loss: 1.1090, acc: 68.92%.\n",
      "--- epoch: 140 ---\n",
      "training loss: 0.1302, acc: 95.77%, dev loss: 0.8745, acc: 77.70%.\n",
      "--- epoch: 141 ---\n",
      "training loss: 0.1224, acc: 95.60%, dev loss: 0.7926, acc: 78.38%.\n",
      "--- epoch: 142 ---\n",
      "training loss: 0.1487, acc: 95.77%, dev loss: 0.8141, acc: 75.00%.\n",
      "--- epoch: 143 ---\n",
      "training loss: 0.1140, acc: 95.43%, dev loss: 0.7923, acc: 79.73%.\n",
      "--- epoch: 144 ---\n",
      "training loss: 0.1219, acc: 95.60%, dev loss: 0.7920, acc: 79.05%.\n",
      "--- epoch: 145 ---\n",
      "training loss: 0.1188, acc: 96.62%, dev loss: 0.8991, acc: 75.68%.\n",
      "--- epoch: 146 ---\n",
      "training loss: 0.1274, acc: 96.28%, dev loss: 1.0529, acc: 72.97%.\n",
      "--- epoch: 147 ---\n",
      "training loss: 0.1353, acc: 94.92%, dev loss: 0.7670, acc: 75.68%.\n",
      "--- epoch: 148 ---\n",
      "training loss: 0.1219, acc: 96.79%, dev loss: 0.9447, acc: 79.73%.\n",
      "--- epoch: 149 ---\n",
      "training loss: 0.1485, acc: 94.92%, dev loss: 0.8109, acc: 77.03%.\n",
      "--- epoch: 150 ---\n",
      "training loss: 0.1453, acc: 95.43%, dev loss: 0.9972, acc: 74.32%.\n",
      "--- epoch: 151 ---\n",
      "training loss: 0.1544, acc: 94.59%, dev loss: 0.7605, acc: 81.08%.\n",
      "--- epoch: 152 ---\n",
      "training loss: 0.1594, acc: 94.75%, dev loss: 0.7220, acc: 79.05%.\n",
      "--- epoch: 153 ---\n",
      "training loss: 0.1217, acc: 95.60%, dev loss: 0.7190, acc: 83.78%.\n",
      "--- epoch: 154 ---\n",
      "training loss: 0.2008, acc: 92.89%, dev loss: 0.7834, acc: 77.70%.\n",
      "--- epoch: 155 ---\n",
      "training loss: 0.2022, acc: 93.40%, dev loss: 0.8995, acc: 76.35%.\n",
      "--- epoch: 156 ---\n",
      "training loss: 0.1713, acc: 94.59%, dev loss: 0.7916, acc: 79.05%.\n",
      "--- epoch: 157 ---\n",
      "training loss: 0.1372, acc: 95.60%, dev loss: 0.7345, acc: 79.05%.\n",
      "--- epoch: 158 ---\n",
      "training loss: 0.1175, acc: 96.62%, dev loss: 0.8250, acc: 77.70%.\n",
      "--- epoch: 159 ---\n",
      "training loss: 0.1493, acc: 94.25%, dev loss: 0.8524, acc: 71.62%.\n",
      "--- epoch: 160 ---\n",
      "training loss: 0.1376, acc: 96.79%, dev loss: 1.0232, acc: 72.97%.\n",
      "--- epoch: 161 ---\n",
      "training loss: 0.1072, acc: 96.79%, dev loss: 0.7895, acc: 78.38%.\n",
      "--- epoch: 162 ---\n",
      "training loss: 0.1474, acc: 94.92%, dev loss: 0.8175, acc: 75.68%.\n",
      "--- epoch: 163 ---\n",
      "training loss: 0.1588, acc: 94.92%, dev loss: 0.9230, acc: 72.97%.\n",
      "--- epoch: 164 ---\n",
      "training loss: 0.1882, acc: 93.40%, dev loss: 1.0010, acc: 70.95%.\n",
      "--- epoch: 165 ---\n",
      "training loss: 0.1964, acc: 93.40%, dev loss: 0.8114, acc: 77.70%.\n",
      "--- epoch: 166 ---\n",
      "training loss: 0.1433, acc: 96.11%, dev loss: 0.7163, acc: 77.70%.\n",
      "--- epoch: 167 ---\n",
      "training loss: 0.1644, acc: 94.42%, dev loss: 0.7026, acc: 77.03%.\n",
      "--- epoch: 168 ---\n",
      "training loss: 0.1276, acc: 96.28%, dev loss: 0.8583, acc: 80.41%.\n",
      "--- epoch: 169 ---\n",
      "training loss: 0.1112, acc: 96.79%, dev loss: 0.8034, acc: 75.68%.\n",
      "--- epoch: 170 ---\n",
      "training loss: 0.1264, acc: 95.94%, dev loss: 0.8170, acc: 77.70%.\n",
      "--- epoch: 171 ---\n",
      "training loss: 0.1474, acc: 96.11%, dev loss: 0.8416, acc: 77.70%.\n",
      "--- epoch: 172 ---\n",
      "training loss: 0.1351, acc: 95.60%, dev loss: 0.6865, acc: 79.73%.\n",
      "--- epoch: 173 ---\n",
      "training loss: 0.1346, acc: 96.45%, dev loss: 0.7526, acc: 76.35%.\n",
      "--- epoch: 174 ---\n",
      "training loss: 0.1442, acc: 95.94%, dev loss: 0.8568, acc: 73.65%.\n",
      "--- epoch: 175 ---\n",
      "training loss: 0.1300, acc: 95.43%, dev loss: 0.9196, acc: 78.38%.\n",
      "--- epoch: 176 ---\n",
      "training loss: 0.1840, acc: 93.57%, dev loss: 0.7865, acc: 81.08%.\n",
      "--- epoch: 177 ---\n",
      "training loss: 0.1199, acc: 95.94%, dev loss: 0.7266, acc: 77.70%.\n",
      "--- epoch: 178 ---\n",
      "training loss: 0.1215, acc: 96.28%, dev loss: 0.8020, acc: 73.65%.\n",
      "--- epoch: 179 ---\n",
      "training loss: 0.0904, acc: 96.62%, dev loss: 0.7966, acc: 81.08%.\n",
      "--- epoch: 180 ---\n",
      "training loss: 0.1021, acc: 97.12%, dev loss: 0.7820, acc: 79.05%.\n",
      "--- epoch: 181 ---\n",
      "training loss: 0.1363, acc: 95.43%, dev loss: 0.7479, acc: 77.03%.\n",
      "--- epoch: 182 ---\n",
      "training loss: 0.0994, acc: 96.79%, dev loss: 0.7924, acc: 80.41%.\n",
      "--- epoch: 183 ---\n",
      "training loss: 0.0923, acc: 96.28%, dev loss: 0.8942, acc: 77.70%.\n",
      "--- epoch: 184 ---\n",
      "training loss: 0.1485, acc: 95.77%, dev loss: 0.8127, acc: 77.03%.\n",
      "--- epoch: 185 ---\n",
      "training loss: 0.1676, acc: 94.42%, dev loss: 0.6920, acc: 80.41%.\n",
      "--- epoch: 186 ---\n",
      "training loss: 0.1450, acc: 95.77%, dev loss: 0.7500, acc: 79.73%.\n",
      "--- epoch: 187 ---\n",
      "training loss: 0.1121, acc: 96.95%, dev loss: 0.6933, acc: 81.76%.\n",
      "--- epoch: 188 ---\n",
      "training loss: 0.1127, acc: 97.12%, dev loss: 0.7771, acc: 83.11%.\n",
      "--- epoch: 189 ---\n",
      "training loss: 0.0897, acc: 97.63%, dev loss: 0.8302, acc: 81.08%.\n",
      "--- epoch: 190 ---\n",
      "training loss: 0.1256, acc: 95.60%, dev loss: 0.7354, acc: 79.05%.\n",
      "--- epoch: 191 ---\n",
      "training loss: 0.1046, acc: 96.79%, dev loss: 0.8182, acc: 76.35%.\n",
      "--- epoch: 192 ---\n",
      "training loss: 0.0935, acc: 96.95%, dev loss: 0.7713, acc: 80.41%.\n",
      "--- epoch: 193 ---\n",
      "training loss: 0.1333, acc: 95.94%, dev loss: 0.7706, acc: 77.03%.\n",
      "--- epoch: 194 ---\n",
      "training loss: 0.1436, acc: 95.43%, dev loss: 0.7817, acc: 79.05%.\n",
      "--- epoch: 195 ---\n",
      "training loss: 0.1488, acc: 95.60%, dev loss: 0.7735, acc: 77.70%.\n",
      "--- epoch: 196 ---\n",
      "training loss: 0.1257, acc: 95.60%, dev loss: 0.8120, acc: 78.38%.\n",
      "--- epoch: 197 ---\n",
      "training loss: 0.1046, acc: 96.79%, dev loss: 0.7451, acc: 77.70%.\n",
      "--- epoch: 198 ---\n",
      "training loss: 0.1086, acc: 96.79%, dev loss: 0.7014, acc: 81.76%.\n",
      "--- epoch: 199 ---\n",
      "training loss: 0.0902, acc: 96.95%, dev loss: 0.8049, acc: 79.05%.\n",
      "--- epoch: 200 ---\n",
      "training loss: 0.1158, acc: 96.45%, dev loss: 0.7375, acc: 79.05%.\n",
      "best dev acc: 0.8378\n"
     ]
    }
   ],
   "source": [
    "net = MyModel()\n",
    "best_net = train(net, train_data_loader, dev_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(best_net, model_path)\n",
    "net.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data_loader):\n",
    "    test_stat = Stat(training=False)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader:\n",
    "            data, labels = batch[0], batch[1]\n",
    "            pred_outputs = model(data)\n",
    "            test_stat.add(pred_outputs, labels, 0)\n",
    "\n",
    "    report = classification_report(\n",
    "        test_stat.labels,\n",
    "        test_stat.pred_labels,\n",
    "        target_names=class_list,\n",
    "        digits=4,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 3, 100, 100]), torch.Size([6]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_test = '../TestImages/'\n",
    "imgs=[]\n",
    "for im in glob.glob(path_test+'/*.jpg'):\n",
    "    img=cv2.imread(im)           \n",
    "    img=cv2.resize(img,(w,h)) \n",
    "    imgs.append(img)  \n",
    "imgs = np.asarray(imgs,np.float32)    \n",
    "\n",
    "test_data = imgs / 255\n",
    "test_label = [x for x in range(len(test_data))]\n",
    "\n",
    "test_data = torch.FloatTensor(test_data).permute(0, 3, 1, 2)\n",
    "test_label = torch.LongTensor(test_label)\n",
    "test_data.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "           bee     1.0000    1.0000    1.0000         1\n",
      "    blackberry     1.0000    1.0000    1.0000         1\n",
      "       blanket     1.0000    1.0000    1.0000         1\n",
      "bougainvilliea     1.0000    1.0000    1.0000         1\n",
      "      bromelia     1.0000    1.0000    1.0000         1\n",
      "      foxglove     1.0000    1.0000    1.0000         1\n",
      "\n",
      "      accuracy                         1.0000         6\n",
      "     macro avg     1.0000    1.0000    1.0000         6\n",
      "  weighted avg     1.0000    1.0000    1.0000         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_set = FlowerDataset(test_data, test_label)\n",
    "test_data_loader = DataLoader(test_set, batch_size=len(test_data), shuffle=False)\n",
    "test(net, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "           bee     1.0000    1.0000    1.0000         1\n",
      "    blackberry     1.0000    1.0000    1.0000         1\n",
      "       blanket     1.0000    1.0000    1.0000         1\n",
      "bougainvilliea     1.0000    1.0000    1.0000         1\n",
      "      bromelia     1.0000    1.0000    1.0000         1\n",
      "      foxglove     1.0000    1.0000    1.0000         1\n",
      "\n",
      "      accuracy                         1.0000         6\n",
      "     macro avg     1.0000    1.0000    1.0000         6\n",
      "  weighted avg     1.0000    1.0000    1.0000         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = MyModel()\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "test(net, test_data_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0a0aef9578cc1edfecec80d6a58c391f944f9223b5cad1c1ac9c6fdf88bcc20"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('basework')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
