{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../flower_photos/'\n",
    "w = 100\n",
    "h = 100\n",
    "c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    cate=[path+x for x in os.listdir(path) if os.path.isdir(path+x)] \n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for idx, folder in enumerate(cate):\n",
    "        for im in glob.glob(folder + '/*.jpg'):\n",
    "            img = cv2.imread(im)\n",
    "            img = cv2.resize(img, (w, h))\n",
    "            imgs.append(img)\n",
    "            labels.append(idx)\n",
    "    return np.asarray(imgs, np.float32), np.asarray(labels, np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([740, 3, 100, 100]), torch.Size([740]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = read_img(path)\n",
    "\n",
    "data = torch.FloatTensor(data).permute(0, 3, 1, 2)\n",
    "label = torch.LongTensor(label)\n",
    "\n",
    "data.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 109\n",
    "np.random.seed(seed)\n",
    "\n",
    "data = data / 255\n",
    "# (x_train, x_val, y_train, y_val) = train_test_split(data, label, test_size=0.20, random_state=seed)\n",
    "# x_train = x_train / 255\n",
    "# x_val = x_val / 255\n",
    "\n",
    "flower_dict = {0:'bee', 1:'blackberry', 2:'blanket', 3:'bougainvilliea', 4:'bromelia', 5:'foxglove'}\n",
    "class_list = ['bee', 'blackberry', 'blanket', 'bougainvilliea', 'bromelia', 'foxglove']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TORCH框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        super(FlowerDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    model\n",
    "    采取和tf一样的CNN架构\n",
    "'''\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.conv_model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(12*12*64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 6),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.conv_model(data)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stat:\n",
    "    def __init__(self, training, writer=None):\n",
    "        self.step = 0\n",
    "        self.loss = []\n",
    "        self.labels = []\n",
    "        self.pred_labels = []\n",
    "        self.training = training\n",
    "        self.writer = writer\n",
    "    \n",
    "    def add(self, pred, labels, loss):\n",
    "        labels = labels.numpy()\n",
    "        pred = pred.detach().numpy()\n",
    "        pred_labels = np.argmax(pred, axis = 1)\n",
    "        self.loss.append(loss)\n",
    "        self.labels.extend(labels)\n",
    "        self.pred_labels.extend(pred_labels)\n",
    "\n",
    "    def log(self):\n",
    "        self.step += 1\n",
    "        acc = accuracy_score(self.labels, self.pred_labels)\n",
    "        loss = sum(self.loss) / len(self.loss)\n",
    "        self.loss = []\n",
    "        self.labels = []\n",
    "        self.pred_labels = []\n",
    "        if not self.writer:\n",
    "            return loss, acc\n",
    "        if self.training:\n",
    "            self.writer.add_scalar('train_loss', loss, self.step)\n",
    "            self.writer.add_scalar('train_acc', acc, self.step)\n",
    "        else:\n",
    "            self.writer.add_scalar('dev_loss', loss, self.step)\n",
    "            self.writer.add_scalar('dev_acc', acc, self.step)\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, dev_data_loader):\n",
    "    loss_func = CrossEntropyLoss()\n",
    "    optimizer = Adam(\n",
    "        model.parameters(),\n",
    "        lr=0.001,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    writer = SummaryWriter('./summary/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n",
    "    train_stat = Stat(training=True, writer=writer)\n",
    "    dev_stat = Stat(training=False, writer=writer)\n",
    "\n",
    "\n",
    "    best_acc, best_net = 0.0, None\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"--- epoch: {epoch + 1} ---\")\n",
    "        for iter, batch in enumerate(train_data_loader):\n",
    "            model.train()\n",
    "            data, labels = batch[0], batch[1]\n",
    "            pred_outputs = model(data)\n",
    "            loss = loss_func(pred_outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_stat.add(pred_outputs, labels, loss.item())\n",
    "            \n",
    "        train_loss, train_acc = train_stat.log()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in dev_data_loader:\n",
    "                data, labels = batch[0], batch[1]\n",
    "                pred_outputs = model(data)\n",
    "                loss = loss_func(pred_outputs, labels)\n",
    "                dev_stat.add(pred_outputs, labels, loss.item())\n",
    "        dev_loss, dev_acc = dev_stat.log()\n",
    "        print(  f\"training loss: {train_loss:.4f}, acc: {train_acc:.2%}, \" \\\n",
    "                f\"dev loss: {dev_loss:.4f}, acc: {dev_acc:.2%}.\")\n",
    "\n",
    "        if dev_acc > best_acc:\n",
    "            best_acc = dev_acc\n",
    "            best_net = deepcopy(model.state_dict())\n",
    "            \n",
    "    print(f\"best dev acc: {best_acc:.4f}\")\n",
    "    return best_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = f'C:\\\\Users\\zzzgry\\Desktop\\midspore_lab2\\model\\\\pytorch\\\\model-'+time.strftime('%m-%d_%H.%M', time.localtime())+'.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = FlowerDataset(x_train, y_train)\n",
    "# dev_set = FlowerDataset(x_val, y_val)\n",
    "\n",
    "# train_data_loader = DataLoader(train_set, 32, True)\n",
    "# dev_data_loader = DataLoader(dev_set, 32, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold:1==============\n",
      "--- epoch: 1 ---\n",
      "training loss: 1.7395, acc: 28.38%, dev loss: 1.6093, acc: 29.73%.\n",
      "--- epoch: 2 ---\n",
      "training loss: 1.5969, acc: 32.43%, dev loss: 1.4152, acc: 39.86%.\n",
      "--- epoch: 3 ---\n",
      "training loss: 1.4848, acc: 42.57%, dev loss: 1.3261, acc: 42.57%.\n",
      "--- epoch: 4 ---\n",
      "training loss: 1.3710, acc: 43.92%, dev loss: 1.2638, acc: 45.95%.\n",
      "--- epoch: 5 ---\n",
      "training loss: 1.3583, acc: 43.58%, dev loss: 1.2432, acc: 43.24%.\n",
      "--- epoch: 6 ---\n",
      "training loss: 1.2851, acc: 47.64%, dev loss: 1.1653, acc: 46.62%.\n",
      "--- epoch: 7 ---\n",
      "training loss: 1.2707, acc: 49.49%, dev loss: 1.2286, acc: 47.97%.\n",
      "--- epoch: 8 ---\n",
      "training loss: 1.2623, acc: 48.82%, dev loss: 1.1514, acc: 54.05%.\n",
      "--- epoch: 9 ---\n",
      "training loss: 1.1676, acc: 53.72%, dev loss: 1.2182, acc: 48.65%.\n",
      "--- epoch: 10 ---\n",
      "training loss: 1.1072, acc: 56.59%, dev loss: 1.0847, acc: 54.05%.\n",
      "--- epoch: 11 ---\n",
      "training loss: 1.0179, acc: 59.46%, dev loss: 1.0240, acc: 61.49%.\n",
      "--- epoch: 12 ---\n",
      "training loss: 0.9845, acc: 60.47%, dev loss: 1.0232, acc: 61.49%.\n",
      "--- epoch: 13 ---\n",
      "training loss: 0.9676, acc: 66.55%, dev loss: 0.9935, acc: 61.49%.\n",
      "--- epoch: 14 ---\n",
      "training loss: 0.9401, acc: 63.51%, dev loss: 0.9865, acc: 60.14%.\n",
      "--- epoch: 15 ---\n",
      "training loss: 0.8916, acc: 68.41%, dev loss: 0.9779, acc: 60.81%.\n",
      "--- epoch: 16 ---\n",
      "training loss: 0.8481, acc: 69.43%, dev loss: 0.9540, acc: 62.84%.\n",
      "--- epoch: 17 ---\n",
      "training loss: 0.8810, acc: 67.06%, dev loss: 0.9468, acc: 64.19%.\n",
      "--- epoch: 18 ---\n",
      "training loss: 0.7541, acc: 70.95%, dev loss: 1.0638, acc: 64.86%.\n",
      "--- epoch: 19 ---\n",
      "training loss: 0.7928, acc: 70.95%, dev loss: 0.9280, acc: 62.84%.\n",
      "--- epoch: 20 ---\n",
      "training loss: 0.7757, acc: 71.11%, dev loss: 0.8897, acc: 64.19%.\n",
      "--- epoch: 21 ---\n",
      "training loss: 0.6876, acc: 75.84%, dev loss: 0.9471, acc: 62.16%.\n",
      "--- epoch: 22 ---\n",
      "training loss: 0.7321, acc: 72.64%, dev loss: 0.9232, acc: 63.51%.\n",
      "--- epoch: 23 ---\n",
      "training loss: 0.6753, acc: 73.65%, dev loss: 0.9240, acc: 64.19%.\n",
      "--- epoch: 24 ---\n",
      "training loss: 0.6208, acc: 77.36%, dev loss: 0.9923, acc: 63.51%.\n",
      "--- epoch: 25 ---\n",
      "training loss: 0.5936, acc: 76.69%, dev loss: 0.9435, acc: 66.89%.\n",
      "--- epoch: 26 ---\n",
      "training loss: 0.6078, acc: 76.18%, dev loss: 0.9351, acc: 63.51%.\n",
      "--- epoch: 27 ---\n",
      "training loss: 0.6379, acc: 75.51%, dev loss: 0.9808, acc: 66.22%.\n",
      "--- epoch: 28 ---\n",
      "training loss: 0.6198, acc: 78.72%, dev loss: 0.8673, acc: 66.22%.\n",
      "--- epoch: 29 ---\n",
      "training loss: 0.5289, acc: 81.59%, dev loss: 0.9069, acc: 68.24%.\n",
      "--- epoch: 30 ---\n",
      "training loss: 0.5769, acc: 78.55%, dev loss: 0.8807, acc: 64.19%.\n",
      "--- epoch: 31 ---\n",
      "training loss: 0.4759, acc: 83.61%, dev loss: 0.9206, acc: 63.51%.\n",
      "--- epoch: 32 ---\n",
      "training loss: 0.5239, acc: 81.59%, dev loss: 0.8671, acc: 66.22%.\n",
      "--- epoch: 33 ---\n",
      "training loss: 0.4309, acc: 83.61%, dev loss: 0.8945, acc: 66.89%.\n",
      "--- epoch: 34 ---\n",
      "training loss: 0.4208, acc: 84.46%, dev loss: 0.9294, acc: 66.22%.\n",
      "--- epoch: 35 ---\n",
      "training loss: 0.4031, acc: 85.81%, dev loss: 0.8935, acc: 65.54%.\n",
      "--- epoch: 36 ---\n",
      "training loss: 0.3786, acc: 86.15%, dev loss: 0.9956, acc: 66.22%.\n",
      "--- epoch: 37 ---\n",
      "training loss: 0.3806, acc: 85.47%, dev loss: 0.9977, acc: 68.24%.\n",
      "--- epoch: 38 ---\n",
      "training loss: 0.4264, acc: 86.99%, dev loss: 1.0102, acc: 64.86%.\n",
      "--- epoch: 39 ---\n",
      "training loss: 0.3803, acc: 86.15%, dev loss: 0.8504, acc: 69.59%.\n",
      "--- epoch: 40 ---\n",
      "training loss: 0.3512, acc: 87.67%, dev loss: 0.8980, acc: 68.92%.\n",
      "--- epoch: 41 ---\n",
      "training loss: 0.2982, acc: 89.53%, dev loss: 0.9578, acc: 67.57%.\n",
      "--- epoch: 42 ---\n",
      "training loss: 0.2910, acc: 89.36%, dev loss: 0.9294, acc: 68.92%.\n",
      "--- epoch: 43 ---\n",
      "training loss: 0.2781, acc: 89.53%, dev loss: 0.9850, acc: 65.54%.\n",
      "--- epoch: 44 ---\n",
      "training loss: 0.2707, acc: 89.86%, dev loss: 0.9587, acc: 66.22%.\n",
      "--- epoch: 45 ---\n",
      "training loss: 0.3319, acc: 87.50%, dev loss: 0.9334, acc: 67.57%.\n",
      "--- epoch: 46 ---\n",
      "training loss: 0.3276, acc: 87.84%, dev loss: 0.8977, acc: 67.57%.\n",
      "--- epoch: 47 ---\n",
      "training loss: 0.2387, acc: 92.74%, dev loss: 0.9837, acc: 72.97%.\n",
      "--- epoch: 48 ---\n",
      "training loss: 0.2446, acc: 91.39%, dev loss: 0.9589, acc: 67.57%.\n",
      "--- epoch: 49 ---\n",
      "training loss: 0.2857, acc: 89.70%, dev loss: 0.9320, acc: 70.95%.\n",
      "--- epoch: 50 ---\n",
      "training loss: 0.2521, acc: 92.23%, dev loss: 1.0217, acc: 66.89%.\n",
      "--- epoch: 51 ---\n",
      "training loss: 0.2693, acc: 91.22%, dev loss: 0.8717, acc: 69.59%.\n",
      "--- epoch: 52 ---\n",
      "training loss: 0.2650, acc: 90.88%, dev loss: 0.8568, acc: 69.59%.\n",
      "--- epoch: 53 ---\n",
      "training loss: 0.2505, acc: 90.37%, dev loss: 0.9657, acc: 68.92%.\n",
      "--- epoch: 54 ---\n",
      "training loss: 0.2673, acc: 92.06%, dev loss: 0.9721, acc: 68.92%.\n",
      "--- epoch: 55 ---\n",
      "training loss: 0.2385, acc: 90.71%, dev loss: 0.9991, acc: 66.89%.\n",
      "--- epoch: 56 ---\n",
      "training loss: 0.2990, acc: 89.53%, dev loss: 1.0360, acc: 65.54%.\n",
      "--- epoch: 57 ---\n",
      "training loss: 0.2433, acc: 90.37%, dev loss: 0.9539, acc: 68.92%.\n",
      "--- epoch: 58 ---\n",
      "training loss: 0.2529, acc: 92.06%, dev loss: 0.9645, acc: 68.92%.\n",
      "--- epoch: 59 ---\n",
      "training loss: 0.2444, acc: 92.23%, dev loss: 0.9427, acc: 69.59%.\n",
      "--- epoch: 60 ---\n",
      "training loss: 0.2459, acc: 91.22%, dev loss: 1.0845, acc: 68.92%.\n",
      "--- epoch: 61 ---\n",
      "training loss: 0.2487, acc: 92.74%, dev loss: 1.0281, acc: 69.59%.\n",
      "--- epoch: 62 ---\n",
      "training loss: 0.2584, acc: 89.70%, dev loss: 0.8928, acc: 68.92%.\n",
      "--- epoch: 63 ---\n",
      "training loss: 0.2485, acc: 91.05%, dev loss: 1.0305, acc: 64.19%.\n",
      "--- epoch: 64 ---\n",
      "training loss: 0.2609, acc: 90.88%, dev loss: 1.0392, acc: 68.24%.\n",
      "--- epoch: 65 ---\n",
      "training loss: 0.2068, acc: 94.43%, dev loss: 0.9220, acc: 68.24%.\n",
      "--- epoch: 66 ---\n",
      "training loss: 0.2567, acc: 90.71%, dev loss: 1.0456, acc: 67.57%.\n",
      "--- epoch: 67 ---\n",
      "training loss: 0.2554, acc: 92.06%, dev loss: 0.9232, acc: 68.92%.\n",
      "--- epoch: 68 ---\n",
      "training loss: 0.2300, acc: 92.57%, dev loss: 0.9435, acc: 68.24%.\n",
      "--- epoch: 69 ---\n",
      "training loss: 0.1999, acc: 93.07%, dev loss: 1.0127, acc: 67.57%.\n",
      "--- epoch: 70 ---\n",
      "training loss: 0.1860, acc: 94.93%, dev loss: 0.9745, acc: 65.54%.\n",
      "--- epoch: 71 ---\n",
      "training loss: 0.2035, acc: 92.74%, dev loss: 1.3066, acc: 69.59%.\n",
      "--- epoch: 72 ---\n",
      "training loss: 0.2184, acc: 92.91%, dev loss: 0.9628, acc: 68.24%.\n",
      "--- epoch: 73 ---\n",
      "training loss: 0.2052, acc: 93.07%, dev loss: 1.0093, acc: 66.89%.\n",
      "--- epoch: 74 ---\n",
      "training loss: 0.2400, acc: 91.89%, dev loss: 0.9410, acc: 70.27%.\n",
      "--- epoch: 75 ---\n",
      "training loss: 0.2017, acc: 93.41%, dev loss: 0.9732, acc: 68.24%.\n",
      "--- epoch: 76 ---\n",
      "training loss: 0.1476, acc: 94.76%, dev loss: 1.0264, acc: 65.54%.\n",
      "--- epoch: 77 ---\n",
      "training loss: 0.1641, acc: 95.27%, dev loss: 1.0635, acc: 68.24%.\n",
      "--- epoch: 78 ---\n",
      "training loss: 0.1847, acc: 93.07%, dev loss: 0.9981, acc: 66.22%.\n",
      "--- epoch: 79 ---\n",
      "training loss: 0.2240, acc: 91.89%, dev loss: 0.9088, acc: 70.27%.\n",
      "--- epoch: 80 ---\n",
      "training loss: 0.1817, acc: 94.43%, dev loss: 1.0558, acc: 68.24%.\n",
      "--- epoch: 81 ---\n",
      "training loss: 0.2203, acc: 93.41%, dev loss: 0.9889, acc: 64.86%.\n",
      "--- epoch: 82 ---\n",
      "training loss: 0.1905, acc: 93.75%, dev loss: 0.9454, acc: 71.62%.\n",
      "--- epoch: 83 ---\n",
      "training loss: 0.2306, acc: 91.55%, dev loss: 0.9710, acc: 66.22%.\n",
      "--- epoch: 84 ---\n",
      "training loss: 0.1699, acc: 94.59%, dev loss: 0.9587, acc: 66.89%.\n",
      "--- epoch: 85 ---\n",
      "training loss: 0.1923, acc: 93.58%, dev loss: 0.9557, acc: 67.57%.\n",
      "--- epoch: 86 ---\n",
      "training loss: 0.1793, acc: 93.92%, dev loss: 0.9388, acc: 66.89%.\n",
      "--- epoch: 87 ---\n",
      "training loss: 0.1766, acc: 94.09%, dev loss: 0.9279, acc: 66.89%.\n",
      "--- epoch: 88 ---\n",
      "training loss: 0.2288, acc: 91.89%, dev loss: 1.0995, acc: 66.22%.\n",
      "--- epoch: 89 ---\n",
      "training loss: 0.1885, acc: 93.58%, dev loss: 0.9939, acc: 66.22%.\n",
      "--- epoch: 90 ---\n",
      "training loss: 0.1972, acc: 92.74%, dev loss: 0.8824, acc: 69.59%.\n",
      "--- epoch: 91 ---\n",
      "training loss: 0.1838, acc: 94.76%, dev loss: 0.9595, acc: 68.92%.\n",
      "--- epoch: 92 ---\n",
      "training loss: 0.1773, acc: 93.92%, dev loss: 0.9847, acc: 68.92%.\n",
      "--- epoch: 93 ---\n",
      "training loss: 0.1748, acc: 93.75%, dev loss: 0.9726, acc: 69.59%.\n",
      "--- epoch: 94 ---\n",
      "training loss: 0.1876, acc: 93.92%, dev loss: 1.0299, acc: 66.22%.\n",
      "--- epoch: 95 ---\n",
      "training loss: 0.1865, acc: 94.26%, dev loss: 0.9329, acc: 72.30%.\n",
      "--- epoch: 96 ---\n",
      "training loss: 0.1823, acc: 94.09%, dev loss: 1.1084, acc: 66.89%.\n",
      "--- epoch: 97 ---\n",
      "training loss: 0.2211, acc: 92.57%, dev loss: 0.9417, acc: 67.57%.\n",
      "--- epoch: 98 ---\n",
      "training loss: 0.1726, acc: 94.26%, dev loss: 1.0917, acc: 68.24%.\n",
      "--- epoch: 99 ---\n",
      "training loss: 0.2093, acc: 93.07%, dev loss: 0.9487, acc: 68.92%.\n",
      "--- epoch: 100 ---\n",
      "training loss: 0.1970, acc: 93.92%, dev loss: 0.9733, acc: 71.62%.\n",
      "best dev acc: 0.7297\n",
      "===========fold:2==============\n",
      "--- epoch: 1 ---\n",
      "training loss: 1.6831, acc: 29.56%, dev loss: 1.5014, acc: 41.89%.\n",
      "--- epoch: 2 ---\n",
      "training loss: 1.4739, acc: 41.89%, dev loss: 1.2673, acc: 43.92%.\n",
      "--- epoch: 3 ---\n",
      "training loss: 1.3465, acc: 43.24%, dev loss: 1.1993, acc: 49.32%.\n",
      "--- epoch: 4 ---\n",
      "training loss: 1.3099, acc: 47.64%, dev loss: 1.2189, acc: 48.65%.\n",
      "--- epoch: 5 ---\n",
      "training loss: 1.2778, acc: 47.80%, dev loss: 1.1100, acc: 54.73%.\n",
      "--- epoch: 6 ---\n",
      "training loss: 1.2020, acc: 52.36%, dev loss: 1.0877, acc: 58.11%.\n",
      "--- epoch: 7 ---\n",
      "training loss: 1.1255, acc: 53.38%, dev loss: 1.0185, acc: 55.41%.\n",
      "--- epoch: 8 ---\n",
      "training loss: 1.1022, acc: 55.91%, dev loss: 0.9728, acc: 64.86%.\n",
      "--- epoch: 9 ---\n",
      "training loss: 1.0054, acc: 63.18%, dev loss: 0.8880, acc: 62.16%.\n",
      "--- epoch: 10 ---\n",
      "training loss: 0.9455, acc: 63.85%, dev loss: 0.9289, acc: 67.57%.\n",
      "--- epoch: 11 ---\n",
      "training loss: 0.9097, acc: 65.71%, dev loss: 0.8631, acc: 64.86%.\n",
      "--- epoch: 12 ---\n",
      "training loss: 0.8735, acc: 67.23%, dev loss: 0.7903, acc: 68.92%.\n",
      "--- epoch: 13 ---\n",
      "training loss: 0.8746, acc: 67.57%, dev loss: 0.9138, acc: 67.57%.\n",
      "--- epoch: 14 ---\n",
      "training loss: 0.8799, acc: 68.07%, dev loss: 0.7863, acc: 70.27%.\n",
      "--- epoch: 15 ---\n",
      "training loss: 0.8201, acc: 68.75%, dev loss: 0.7577, acc: 72.30%.\n",
      "--- epoch: 16 ---\n",
      "training loss: 0.7607, acc: 70.44%, dev loss: 0.7376, acc: 73.65%.\n",
      "--- epoch: 17 ---\n",
      "training loss: 0.7535, acc: 71.28%, dev loss: 0.7392, acc: 69.59%.\n",
      "--- epoch: 18 ---\n",
      "training loss: 0.7571, acc: 72.30%, dev loss: 0.7442, acc: 72.30%.\n",
      "--- epoch: 19 ---\n",
      "training loss: 0.7202, acc: 72.64%, dev loss: 0.7382, acc: 72.30%.\n",
      "--- epoch: 20 ---\n",
      "training loss: 0.7083, acc: 74.49%, dev loss: 0.7082, acc: 72.97%.\n",
      "--- epoch: 21 ---\n",
      "training loss: 0.6935, acc: 72.80%, dev loss: 0.6821, acc: 75.00%.\n",
      "--- epoch: 22 ---\n",
      "training loss: 0.6670, acc: 74.83%, dev loss: 0.6993, acc: 72.30%.\n",
      "--- epoch: 23 ---\n",
      "training loss: 0.6734, acc: 77.70%, dev loss: 0.6982, acc: 72.30%.\n",
      "--- epoch: 24 ---\n",
      "training loss: 0.6203, acc: 76.69%, dev loss: 0.6773, acc: 75.00%.\n",
      "--- epoch: 25 ---\n",
      "training loss: 0.6701, acc: 77.20%, dev loss: 0.6584, acc: 74.32%.\n",
      "--- epoch: 26 ---\n",
      "training loss: 0.6411, acc: 75.34%, dev loss: 0.6864, acc: 74.32%.\n",
      "--- epoch: 27 ---\n",
      "training loss: 0.5752, acc: 77.53%, dev loss: 0.6453, acc: 75.00%.\n",
      "--- epoch: 28 ---\n",
      "training loss: 0.5695, acc: 78.89%, dev loss: 0.6513, acc: 72.30%.\n",
      "--- epoch: 29 ---\n",
      "training loss: 0.6296, acc: 75.84%, dev loss: 0.5925, acc: 77.03%.\n",
      "--- epoch: 30 ---\n",
      "training loss: 0.5455, acc: 78.89%, dev loss: 0.7021, acc: 72.30%.\n",
      "--- epoch: 31 ---\n",
      "training loss: 0.5717, acc: 80.41%, dev loss: 0.6698, acc: 73.65%.\n",
      "--- epoch: 32 ---\n",
      "training loss: 0.5485, acc: 79.39%, dev loss: 0.6162, acc: 77.70%.\n",
      "--- epoch: 33 ---\n",
      "training loss: 0.5324, acc: 78.21%, dev loss: 0.5914, acc: 75.00%.\n",
      "--- epoch: 34 ---\n",
      "training loss: 0.4991, acc: 82.09%, dev loss: 0.5813, acc: 78.38%.\n",
      "--- epoch: 35 ---\n",
      "training loss: 0.5124, acc: 80.74%, dev loss: 0.6087, acc: 77.03%.\n",
      "--- epoch: 36 ---\n",
      "training loss: 0.4786, acc: 82.09%, dev loss: 0.5970, acc: 76.35%.\n",
      "--- epoch: 37 ---\n",
      "training loss: 0.4904, acc: 83.11%, dev loss: 0.6115, acc: 74.32%.\n",
      "--- epoch: 38 ---\n",
      "training loss: 0.4563, acc: 83.45%, dev loss: 0.6282, acc: 75.68%.\n",
      "--- epoch: 39 ---\n",
      "training loss: 0.4134, acc: 84.80%, dev loss: 0.5585, acc: 76.35%.\n",
      "--- epoch: 40 ---\n",
      "training loss: 0.4427, acc: 83.28%, dev loss: 0.4914, acc: 78.38%.\n",
      "--- epoch: 41 ---\n",
      "training loss: 0.4028, acc: 85.81%, dev loss: 0.5643, acc: 77.70%.\n",
      "--- epoch: 42 ---\n",
      "training loss: 0.4220, acc: 84.46%, dev loss: 0.4970, acc: 81.76%.\n",
      "--- epoch: 43 ---\n",
      "training loss: 0.3675, acc: 87.16%, dev loss: 0.5013, acc: 80.41%.\n",
      "--- epoch: 44 ---\n",
      "training loss: 0.3428, acc: 86.99%, dev loss: 0.4648, acc: 80.41%.\n",
      "--- epoch: 45 ---\n",
      "training loss: 0.3603, acc: 87.16%, dev loss: 0.5057, acc: 78.38%.\n",
      "--- epoch: 46 ---\n",
      "training loss: 0.3595, acc: 88.68%, dev loss: 0.5402, acc: 78.38%.\n",
      "--- epoch: 47 ---\n",
      "training loss: 0.3479, acc: 88.51%, dev loss: 0.4888, acc: 79.73%.\n",
      "--- epoch: 48 ---\n",
      "training loss: 0.3420, acc: 86.99%, dev loss: 0.5024, acc: 79.73%.\n",
      "--- epoch: 49 ---\n",
      "training loss: 0.3775, acc: 85.64%, dev loss: 0.5229, acc: 78.38%.\n",
      "--- epoch: 50 ---\n",
      "training loss: 0.3732, acc: 86.82%, dev loss: 0.4665, acc: 79.73%.\n",
      "--- epoch: 51 ---\n",
      "training loss: 0.3208, acc: 88.85%, dev loss: 0.4707, acc: 80.41%.\n",
      "--- epoch: 52 ---\n",
      "training loss: 0.3154, acc: 90.54%, dev loss: 0.4964, acc: 79.73%.\n",
      "--- epoch: 53 ---\n",
      "training loss: 0.3097, acc: 88.34%, dev loss: 0.4609, acc: 81.76%.\n",
      "--- epoch: 54 ---\n",
      "training loss: 0.2846, acc: 91.55%, dev loss: 0.5348, acc: 78.38%.\n",
      "--- epoch: 55 ---\n",
      "training loss: 0.2918, acc: 90.37%, dev loss: 0.4737, acc: 79.05%.\n",
      "--- epoch: 56 ---\n",
      "training loss: 0.2863, acc: 89.86%, dev loss: 0.5713, acc: 79.05%.\n",
      "--- epoch: 57 ---\n",
      "training loss: 0.3007, acc: 88.85%, dev loss: 0.5234, acc: 78.38%.\n",
      "--- epoch: 58 ---\n",
      "training loss: 0.2927, acc: 89.19%, dev loss: 0.4252, acc: 82.43%.\n",
      "--- epoch: 59 ---\n",
      "training loss: 0.2918, acc: 90.20%, dev loss: 0.5007, acc: 77.70%.\n",
      "--- epoch: 60 ---\n",
      "training loss: 0.2600, acc: 91.05%, dev loss: 0.5240, acc: 77.70%.\n",
      "--- epoch: 61 ---\n",
      "training loss: 0.2305, acc: 91.89%, dev loss: 0.4896, acc: 80.41%.\n",
      "--- epoch: 62 ---\n",
      "training loss: 0.2476, acc: 91.39%, dev loss: 0.4808, acc: 79.73%.\n",
      "--- epoch: 63 ---\n",
      "training loss: 0.2712, acc: 89.86%, dev loss: 0.4718, acc: 82.43%.\n",
      "--- epoch: 64 ---\n",
      "training loss: 0.2309, acc: 92.91%, dev loss: 0.5021, acc: 76.35%.\n",
      "--- epoch: 65 ---\n",
      "training loss: 0.2230, acc: 91.39%, dev loss: 0.4951, acc: 78.38%.\n",
      "--- epoch: 66 ---\n",
      "training loss: 0.2212, acc: 91.39%, dev loss: 0.6058, acc: 75.68%.\n",
      "--- epoch: 67 ---\n",
      "training loss: 0.2051, acc: 93.41%, dev loss: 0.4513, acc: 81.08%.\n",
      "--- epoch: 68 ---\n",
      "training loss: 0.2115, acc: 91.89%, dev loss: 0.5620, acc: 76.35%.\n",
      "--- epoch: 69 ---\n",
      "training loss: 0.2658, acc: 91.55%, dev loss: 0.4416, acc: 79.05%.\n",
      "--- epoch: 70 ---\n",
      "training loss: 0.2450, acc: 91.22%, dev loss: 0.4219, acc: 80.41%.\n",
      "--- epoch: 71 ---\n",
      "training loss: 0.2021, acc: 92.57%, dev loss: 0.4655, acc: 78.38%.\n",
      "--- epoch: 72 ---\n",
      "training loss: 0.1889, acc: 95.10%, dev loss: 0.5166, acc: 78.38%.\n",
      "--- epoch: 73 ---\n",
      "training loss: 0.2143, acc: 93.41%, dev loss: 0.4566, acc: 83.11%.\n",
      "--- epoch: 74 ---\n",
      "training loss: 0.1983, acc: 93.75%, dev loss: 0.4531, acc: 82.43%.\n",
      "--- epoch: 75 ---\n",
      "training loss: 0.2195, acc: 92.74%, dev loss: 0.4940, acc: 83.11%.\n",
      "--- epoch: 76 ---\n",
      "training loss: 0.2320, acc: 91.89%, dev loss: 0.4899, acc: 81.08%.\n",
      "--- epoch: 77 ---\n",
      "training loss: 0.2030, acc: 93.41%, dev loss: 0.4541, acc: 83.78%.\n",
      "--- epoch: 78 ---\n",
      "training loss: 0.1888, acc: 93.58%, dev loss: 0.5394, acc: 80.41%.\n",
      "--- epoch: 79 ---\n",
      "training loss: 0.2005, acc: 93.24%, dev loss: 0.4591, acc: 79.73%.\n",
      "--- epoch: 80 ---\n",
      "training loss: 0.1840, acc: 94.43%, dev loss: 0.4927, acc: 80.41%.\n",
      "--- epoch: 81 ---\n",
      "training loss: 0.2027, acc: 93.07%, dev loss: 0.5120, acc: 77.70%.\n",
      "--- epoch: 82 ---\n",
      "training loss: 0.2467, acc: 90.71%, dev loss: 0.5283, acc: 81.76%.\n",
      "--- epoch: 83 ---\n",
      "training loss: 0.1983, acc: 93.58%, dev loss: 0.5062, acc: 81.08%.\n",
      "--- epoch: 84 ---\n",
      "training loss: 0.1768, acc: 93.24%, dev loss: 0.4396, acc: 83.78%.\n",
      "--- epoch: 85 ---\n",
      "training loss: 0.2195, acc: 92.74%, dev loss: 0.4897, acc: 81.08%.\n",
      "--- epoch: 86 ---\n",
      "training loss: 0.1989, acc: 93.92%, dev loss: 0.5237, acc: 78.38%.\n",
      "--- epoch: 87 ---\n",
      "training loss: 0.1915, acc: 93.41%, dev loss: 0.4876, acc: 82.43%.\n",
      "--- epoch: 88 ---\n",
      "training loss: 0.1992, acc: 94.59%, dev loss: 0.4616, acc: 80.41%.\n",
      "--- epoch: 89 ---\n",
      "training loss: 0.1491, acc: 95.10%, dev loss: 0.4329, acc: 83.11%.\n",
      "--- epoch: 90 ---\n",
      "training loss: 0.1853, acc: 92.57%, dev loss: 0.4204, acc: 83.78%.\n",
      "--- epoch: 91 ---\n",
      "training loss: 0.1946, acc: 93.24%, dev loss: 0.4332, acc: 85.14%.\n",
      "--- epoch: 92 ---\n",
      "training loss: 0.1687, acc: 94.93%, dev loss: 0.5203, acc: 81.08%.\n",
      "--- epoch: 93 ---\n",
      "training loss: 0.1529, acc: 94.76%, dev loss: 0.4343, acc: 81.08%.\n",
      "--- epoch: 94 ---\n",
      "training loss: 0.1352, acc: 95.95%, dev loss: 0.5405, acc: 79.73%.\n",
      "--- epoch: 95 ---\n",
      "training loss: 0.2040, acc: 93.07%, dev loss: 0.4574, acc: 82.43%.\n",
      "--- epoch: 96 ---\n",
      "training loss: 0.2256, acc: 92.40%, dev loss: 0.5435, acc: 77.70%.\n",
      "--- epoch: 97 ---\n",
      "training loss: 0.1807, acc: 93.24%, dev loss: 0.4859, acc: 83.11%.\n",
      "--- epoch: 98 ---\n",
      "training loss: 0.1557, acc: 95.44%, dev loss: 0.4662, acc: 84.46%.\n",
      "--- epoch: 99 ---\n",
      "training loss: 0.1757, acc: 94.43%, dev loss: 0.5226, acc: 77.03%.\n",
      "--- epoch: 100 ---\n",
      "training loss: 0.2084, acc: 93.24%, dev loss: 0.4382, acc: 81.76%.\n",
      "best dev acc: 0.8514\n",
      "===========fold:3==============\n",
      "--- epoch: 1 ---\n",
      "training loss: 1.7050, acc: 30.41%, dev loss: 1.6223, acc: 43.24%.\n",
      "--- epoch: 2 ---\n",
      "training loss: 1.5014, acc: 40.03%, dev loss: 1.3371, acc: 42.57%.\n",
      "--- epoch: 3 ---\n",
      "training loss: 1.3985, acc: 42.40%, dev loss: 1.2903, acc: 47.97%.\n",
      "--- epoch: 4 ---\n",
      "training loss: 1.3097, acc: 46.79%, dev loss: 1.1956, acc: 49.32%.\n",
      "--- epoch: 5 ---\n",
      "training loss: 1.2679, acc: 51.01%, dev loss: 1.1349, acc: 52.70%.\n",
      "--- epoch: 6 ---\n",
      "training loss: 1.1560, acc: 54.56%, dev loss: 1.1188, acc: 58.78%.\n",
      "--- epoch: 7 ---\n",
      "training loss: 1.1115, acc: 57.09%, dev loss: 1.0154, acc: 56.08%.\n",
      "--- epoch: 8 ---\n",
      "training loss: 0.9969, acc: 64.70%, dev loss: 0.9293, acc: 62.16%.\n",
      "--- epoch: 9 ---\n",
      "training loss: 1.0462, acc: 61.99%, dev loss: 0.9530, acc: 63.51%.\n",
      "--- epoch: 10 ---\n",
      "training loss: 0.9507, acc: 65.71%, dev loss: 0.8552, acc: 68.92%.\n",
      "--- epoch: 11 ---\n",
      "training loss: 0.8795, acc: 67.23%, dev loss: 0.8078, acc: 70.95%.\n",
      "--- epoch: 12 ---\n",
      "training loss: 0.8586, acc: 68.07%, dev loss: 0.8644, acc: 67.57%.\n",
      "--- epoch: 13 ---\n",
      "training loss: 0.8104, acc: 70.27%, dev loss: 0.7689, acc: 70.27%.\n",
      "--- epoch: 14 ---\n",
      "training loss: 0.7089, acc: 75.84%, dev loss: 0.7927, acc: 68.24%.\n",
      "--- epoch: 15 ---\n",
      "training loss: 0.7357, acc: 71.79%, dev loss: 0.7548, acc: 68.92%.\n",
      "--- epoch: 16 ---\n",
      "training loss: 0.7410, acc: 73.65%, dev loss: 0.8030, acc: 68.92%.\n",
      "--- epoch: 17 ---\n",
      "training loss: 0.6959, acc: 74.32%, dev loss: 0.8226, acc: 68.92%.\n",
      "--- epoch: 18 ---\n",
      "training loss: 0.6345, acc: 76.35%, dev loss: 0.8213, acc: 68.24%.\n",
      "--- epoch: 19 ---\n",
      "training loss: 0.6718, acc: 73.99%, dev loss: 0.7749, acc: 68.92%.\n",
      "--- epoch: 20 ---\n",
      "training loss: 0.6343, acc: 75.68%, dev loss: 0.7566, acc: 66.22%.\n",
      "--- epoch: 21 ---\n",
      "training loss: 0.6206, acc: 76.35%, dev loss: 0.7713, acc: 70.27%.\n",
      "--- epoch: 22 ---\n",
      "training loss: 0.5833, acc: 75.51%, dev loss: 0.6835, acc: 73.65%.\n",
      "--- epoch: 23 ---\n",
      "training loss: 0.5589, acc: 79.22%, dev loss: 0.6762, acc: 72.30%.\n",
      "--- epoch: 24 ---\n",
      "training loss: 0.5688, acc: 80.24%, dev loss: 0.6869, acc: 76.35%.\n",
      "--- epoch: 25 ---\n",
      "training loss: 0.5249, acc: 80.24%, dev loss: 0.7737, acc: 71.62%.\n",
      "--- epoch: 26 ---\n",
      "training loss: 0.5015, acc: 80.74%, dev loss: 0.7199, acc: 73.65%.\n",
      "--- epoch: 27 ---\n",
      "training loss: 0.4735, acc: 82.09%, dev loss: 0.6630, acc: 74.32%.\n",
      "--- epoch: 28 ---\n",
      "training loss: 0.4711, acc: 82.60%, dev loss: 0.6621, acc: 73.65%.\n",
      "--- epoch: 29 ---\n",
      "training loss: 0.4557, acc: 83.78%, dev loss: 0.8718, acc: 66.22%.\n",
      "--- epoch: 30 ---\n",
      "training loss: 0.4440, acc: 84.80%, dev loss: 0.8177, acc: 72.97%.\n",
      "--- epoch: 31 ---\n",
      "training loss: 0.5302, acc: 80.07%, dev loss: 0.6759, acc: 72.97%.\n",
      "--- epoch: 32 ---\n",
      "training loss: 0.4683, acc: 82.77%, dev loss: 0.6707, acc: 71.62%.\n",
      "--- epoch: 33 ---\n",
      "training loss: 0.3643, acc: 87.16%, dev loss: 0.6186, acc: 75.68%.\n",
      "--- epoch: 34 ---\n",
      "training loss: 0.3947, acc: 84.97%, dev loss: 0.6922, acc: 71.62%.\n",
      "--- epoch: 35 ---\n",
      "training loss: 0.3462, acc: 86.15%, dev loss: 0.6975, acc: 71.62%.\n",
      "--- epoch: 36 ---\n",
      "training loss: 0.3936, acc: 85.64%, dev loss: 0.7141, acc: 70.95%.\n",
      "--- epoch: 37 ---\n",
      "training loss: 0.3979, acc: 86.32%, dev loss: 0.6683, acc: 71.62%.\n",
      "--- epoch: 38 ---\n",
      "training loss: 0.3325, acc: 89.19%, dev loss: 0.6604, acc: 72.97%.\n",
      "--- epoch: 39 ---\n",
      "training loss: 0.3319, acc: 89.02%, dev loss: 0.5899, acc: 77.03%.\n",
      "--- epoch: 40 ---\n",
      "training loss: 0.2925, acc: 89.36%, dev loss: 0.7166, acc: 76.35%.\n",
      "--- epoch: 41 ---\n",
      "training loss: 0.3148, acc: 89.02%, dev loss: 0.8125, acc: 70.27%.\n",
      "--- epoch: 42 ---\n",
      "training loss: 0.3407, acc: 89.70%, dev loss: 0.6704, acc: 73.65%.\n",
      "--- epoch: 43 ---\n",
      "training loss: 0.2850, acc: 90.54%, dev loss: 0.7469, acc: 71.62%.\n",
      "--- epoch: 44 ---\n",
      "training loss: 0.3015, acc: 88.85%, dev loss: 0.7240, acc: 70.95%.\n",
      "--- epoch: 45 ---\n",
      "training loss: 0.3087, acc: 89.86%, dev loss: 0.6940, acc: 76.35%.\n",
      "--- epoch: 46 ---\n",
      "training loss: 0.2740, acc: 89.70%, dev loss: 0.6271, acc: 76.35%.\n",
      "--- epoch: 47 ---\n",
      "training loss: 0.2505, acc: 91.39%, dev loss: 0.7214, acc: 75.00%.\n",
      "--- epoch: 48 ---\n",
      "training loss: 0.2817, acc: 91.39%, dev loss: 0.6472, acc: 77.03%.\n",
      "--- epoch: 49 ---\n",
      "training loss: 0.2780, acc: 91.39%, dev loss: 0.6496, acc: 77.03%.\n",
      "--- epoch: 50 ---\n",
      "training loss: 0.2507, acc: 92.23%, dev loss: 0.7365, acc: 72.30%.\n",
      "--- epoch: 51 ---\n",
      "training loss: 0.2692, acc: 90.03%, dev loss: 0.6981, acc: 72.30%.\n",
      "--- epoch: 52 ---\n",
      "training loss: 0.2524, acc: 92.06%, dev loss: 0.6739, acc: 75.00%.\n",
      "--- epoch: 53 ---\n",
      "training loss: 0.2559, acc: 91.72%, dev loss: 0.6861, acc: 74.32%.\n",
      "--- epoch: 54 ---\n",
      "training loss: 0.1935, acc: 94.76%, dev loss: 0.6299, acc: 75.00%.\n",
      "--- epoch: 55 ---\n",
      "training loss: 0.2362, acc: 91.39%, dev loss: 0.7515, acc: 73.65%.\n",
      "--- epoch: 56 ---\n",
      "training loss: 0.2370, acc: 93.07%, dev loss: 0.7823, acc: 74.32%.\n",
      "--- epoch: 57 ---\n",
      "training loss: 0.2189, acc: 92.57%, dev loss: 0.8554, acc: 72.97%.\n",
      "--- epoch: 58 ---\n",
      "training loss: 0.2398, acc: 92.06%, dev loss: 0.6649, acc: 74.32%.\n",
      "--- epoch: 59 ---\n",
      "training loss: 0.2254, acc: 91.89%, dev loss: 0.6014, acc: 75.68%.\n",
      "--- epoch: 60 ---\n",
      "training loss: 0.2185, acc: 92.23%, dev loss: 0.7106, acc: 71.62%.\n",
      "--- epoch: 61 ---\n",
      "training loss: 0.2141, acc: 93.07%, dev loss: 0.6442, acc: 72.97%.\n",
      "--- epoch: 62 ---\n",
      "training loss: 0.1922, acc: 92.91%, dev loss: 0.6856, acc: 70.95%.\n",
      "--- epoch: 63 ---\n",
      "training loss: 0.1812, acc: 94.26%, dev loss: 0.6829, acc: 71.62%.\n",
      "--- epoch: 64 ---\n",
      "training loss: 0.1661, acc: 94.09%, dev loss: 0.7246, acc: 73.65%.\n",
      "--- epoch: 65 ---\n",
      "training loss: 0.2305, acc: 92.57%, dev loss: 0.6017, acc: 75.68%.\n",
      "--- epoch: 66 ---\n",
      "training loss: 0.2031, acc: 94.09%, dev loss: 0.6297, acc: 75.68%.\n",
      "--- epoch: 67 ---\n",
      "training loss: 0.2313, acc: 93.07%, dev loss: 0.5941, acc: 77.03%.\n",
      "--- epoch: 68 ---\n",
      "training loss: 0.1842, acc: 94.76%, dev loss: 0.5767, acc: 76.35%.\n",
      "--- epoch: 69 ---\n",
      "training loss: 0.1768, acc: 94.09%, dev loss: 0.6468, acc: 77.03%.\n",
      "--- epoch: 70 ---\n",
      "training loss: 0.1452, acc: 95.61%, dev loss: 0.6364, acc: 75.68%.\n",
      "--- epoch: 71 ---\n",
      "training loss: 0.2230, acc: 91.55%, dev loss: 0.6729, acc: 75.68%.\n",
      "--- epoch: 72 ---\n",
      "training loss: 0.2198, acc: 92.74%, dev loss: 0.7661, acc: 75.00%.\n",
      "--- epoch: 73 ---\n",
      "training loss: 0.1779, acc: 93.41%, dev loss: 0.5956, acc: 79.05%.\n",
      "--- epoch: 74 ---\n",
      "training loss: 0.1882, acc: 93.24%, dev loss: 0.6275, acc: 79.73%.\n",
      "--- epoch: 75 ---\n",
      "training loss: 0.2158, acc: 93.24%, dev loss: 0.6769, acc: 79.05%.\n",
      "--- epoch: 76 ---\n",
      "training loss: 0.1760, acc: 93.41%, dev loss: 0.6575, acc: 75.68%.\n",
      "--- epoch: 77 ---\n",
      "training loss: 0.1758, acc: 93.75%, dev loss: 0.6114, acc: 75.00%.\n",
      "--- epoch: 78 ---\n",
      "training loss: 0.2384, acc: 92.23%, dev loss: 0.6035, acc: 77.03%.\n",
      "--- epoch: 79 ---\n",
      "training loss: 0.1668, acc: 94.09%, dev loss: 0.5670, acc: 79.05%.\n",
      "--- epoch: 80 ---\n",
      "training loss: 0.1633, acc: 94.43%, dev loss: 0.5894, acc: 77.03%.\n",
      "--- epoch: 81 ---\n",
      "training loss: 0.1960, acc: 93.58%, dev loss: 0.6582, acc: 76.35%.\n",
      "--- epoch: 82 ---\n",
      "training loss: 0.1989, acc: 93.07%, dev loss: 0.6513, acc: 75.00%.\n",
      "--- epoch: 83 ---\n",
      "training loss: 0.1921, acc: 93.92%, dev loss: 0.6145, acc: 75.68%.\n",
      "--- epoch: 84 ---\n",
      "training loss: 0.2213, acc: 93.41%, dev loss: 0.5615, acc: 78.38%.\n",
      "--- epoch: 85 ---\n",
      "training loss: 0.2116, acc: 92.91%, dev loss: 0.5986, acc: 79.05%.\n",
      "--- epoch: 86 ---\n",
      "training loss: 0.1762, acc: 94.43%, dev loss: 0.6829, acc: 80.41%.\n",
      "--- epoch: 87 ---\n",
      "training loss: 0.1500, acc: 94.43%, dev loss: 0.6616, acc: 76.35%.\n",
      "--- epoch: 88 ---\n",
      "training loss: 0.1400, acc: 95.95%, dev loss: 0.6753, acc: 77.03%.\n",
      "--- epoch: 89 ---\n",
      "training loss: 0.1569, acc: 95.10%, dev loss: 0.5748, acc: 79.05%.\n",
      "--- epoch: 90 ---\n",
      "training loss: 0.1592, acc: 94.43%, dev loss: 0.5616, acc: 79.73%.\n",
      "--- epoch: 91 ---\n",
      "training loss: 0.1469, acc: 95.95%, dev loss: 0.6179, acc: 77.70%.\n",
      "--- epoch: 92 ---\n",
      "training loss: 0.1608, acc: 94.93%, dev loss: 0.5205, acc: 81.76%.\n",
      "--- epoch: 93 ---\n",
      "training loss: 0.1434, acc: 96.11%, dev loss: 0.5998, acc: 78.38%.\n",
      "--- epoch: 94 ---\n",
      "training loss: 0.1762, acc: 94.59%, dev loss: 0.5846, acc: 79.73%.\n",
      "--- epoch: 95 ---\n",
      "training loss: 0.1674, acc: 94.26%, dev loss: 0.5889, acc: 77.70%.\n",
      "--- epoch: 96 ---\n",
      "training loss: 0.1767, acc: 93.92%, dev loss: 0.6139, acc: 77.03%.\n",
      "--- epoch: 97 ---\n",
      "training loss: 0.1918, acc: 94.76%, dev loss: 0.5633, acc: 79.05%.\n",
      "--- epoch: 98 ---\n",
      "training loss: 0.1638, acc: 95.78%, dev loss: 0.5511, acc: 79.05%.\n",
      "--- epoch: 99 ---\n",
      "training loss: 0.1667, acc: 94.59%, dev loss: 0.5513, acc: 79.73%.\n",
      "--- epoch: 100 ---\n",
      "training loss: 0.1506, acc: 95.27%, dev loss: 0.5849, acc: 79.05%.\n",
      "best dev acc: 0.8176\n",
      "===========fold:4==============\n",
      "--- epoch: 1 ---\n",
      "training loss: 1.7162, acc: 27.36%, dev loss: 1.6251, acc: 41.22%.\n",
      "--- epoch: 2 ---\n",
      "training loss: 1.5586, acc: 39.19%, dev loss: 1.3457, acc: 43.92%.\n",
      "--- epoch: 3 ---\n",
      "training loss: 1.3862, acc: 43.92%, dev loss: 1.2677, acc: 48.65%.\n",
      "--- epoch: 4 ---\n",
      "training loss: 1.2986, acc: 46.28%, dev loss: 1.3777, acc: 43.24%.\n",
      "--- epoch: 5 ---\n",
      "training loss: 1.2512, acc: 51.18%, dev loss: 1.1034, acc: 52.70%.\n",
      "--- epoch: 6 ---\n",
      "training loss: 1.0972, acc: 57.77%, dev loss: 1.0325, acc: 59.46%.\n",
      "--- epoch: 7 ---\n",
      "training loss: 1.0395, acc: 62.33%, dev loss: 1.0038, acc: 60.14%.\n",
      "--- epoch: 8 ---\n",
      "training loss: 0.9882, acc: 61.82%, dev loss: 1.0162, acc: 64.19%.\n",
      "--- epoch: 9 ---\n",
      "training loss: 1.0463, acc: 61.82%, dev loss: 0.9593, acc: 64.19%.\n",
      "--- epoch: 10 ---\n",
      "training loss: 0.9230, acc: 65.37%, dev loss: 0.9833, acc: 66.22%.\n",
      "--- epoch: 11 ---\n",
      "training loss: 0.9030, acc: 66.72%, dev loss: 0.9513, acc: 62.84%.\n",
      "--- epoch: 12 ---\n",
      "training loss: 0.9034, acc: 67.40%, dev loss: 0.9885, acc: 56.76%.\n",
      "--- epoch: 13 ---\n",
      "training loss: 0.8422, acc: 69.09%, dev loss: 0.8793, acc: 67.57%.\n",
      "--- epoch: 14 ---\n",
      "training loss: 0.8108, acc: 70.27%, dev loss: 0.8863, acc: 65.54%.\n",
      "--- epoch: 15 ---\n",
      "training loss: 0.8373, acc: 68.58%, dev loss: 0.9527, acc: 58.78%.\n",
      "--- epoch: 16 ---\n",
      "training loss: 0.7928, acc: 69.26%, dev loss: 0.8640, acc: 68.92%.\n",
      "--- epoch: 17 ---\n",
      "training loss: 0.7728, acc: 70.44%, dev loss: 0.9342, acc: 63.51%.\n",
      "--- epoch: 18 ---\n",
      "training loss: 0.7518, acc: 73.31%, dev loss: 0.8291, acc: 70.27%.\n",
      "--- epoch: 19 ---\n",
      "training loss: 0.7089, acc: 72.30%, dev loss: 0.9271, acc: 63.51%.\n",
      "--- epoch: 20 ---\n",
      "training loss: 0.7058, acc: 73.99%, dev loss: 0.8217, acc: 67.57%.\n",
      "--- epoch: 21 ---\n",
      "training loss: 0.6912, acc: 73.14%, dev loss: 0.7890, acc: 73.65%.\n",
      "--- epoch: 22 ---\n",
      "training loss: 0.6606, acc: 75.17%, dev loss: 0.8664, acc: 66.22%.\n",
      "--- epoch: 23 ---\n",
      "training loss: 0.6550, acc: 75.34%, dev loss: 0.8783, acc: 67.57%.\n",
      "--- epoch: 24 ---\n",
      "training loss: 0.6646, acc: 76.35%, dev loss: 0.8739, acc: 71.62%.\n",
      "--- epoch: 25 ---\n",
      "training loss: 0.6060, acc: 76.86%, dev loss: 0.8186, acc: 70.95%.\n",
      "--- epoch: 26 ---\n",
      "training loss: 0.6079, acc: 77.53%, dev loss: 0.7870, acc: 70.95%.\n",
      "--- epoch: 27 ---\n",
      "training loss: 0.5647, acc: 79.56%, dev loss: 0.7811, acc: 72.30%.\n",
      "--- epoch: 28 ---\n",
      "training loss: 0.5261, acc: 80.41%, dev loss: 0.8304, acc: 68.24%.\n",
      "--- epoch: 29 ---\n",
      "training loss: 0.5460, acc: 79.05%, dev loss: 0.7681, acc: 70.27%.\n",
      "--- epoch: 30 ---\n",
      "training loss: 0.5280, acc: 81.59%, dev loss: 0.8506, acc: 70.27%.\n",
      "--- epoch: 31 ---\n",
      "training loss: 0.5668, acc: 78.55%, dev loss: 0.8519, acc: 67.57%.\n",
      "--- epoch: 32 ---\n",
      "training loss: 0.5143, acc: 81.25%, dev loss: 0.7518, acc: 72.30%.\n",
      "--- epoch: 33 ---\n",
      "training loss: 0.5127, acc: 80.57%, dev loss: 0.8333, acc: 69.59%.\n",
      "--- epoch: 34 ---\n",
      "training loss: 0.4912, acc: 81.59%, dev loss: 0.8832, acc: 63.51%.\n",
      "--- epoch: 35 ---\n",
      "training loss: 0.4987, acc: 82.09%, dev loss: 0.7477, acc: 73.65%.\n",
      "--- epoch: 36 ---\n",
      "training loss: 0.4528, acc: 84.63%, dev loss: 0.7966, acc: 66.89%.\n",
      "--- epoch: 37 ---\n",
      "training loss: 0.4275, acc: 85.47%, dev loss: 0.8170, acc: 71.62%.\n",
      "--- epoch: 38 ---\n",
      "training loss: 0.4437, acc: 83.11%, dev loss: 0.7851, acc: 72.30%.\n",
      "--- epoch: 39 ---\n",
      "training loss: 0.4757, acc: 83.45%, dev loss: 0.8703, acc: 70.27%.\n",
      "--- epoch: 40 ---\n",
      "training loss: 0.4632, acc: 81.93%, dev loss: 0.9129, acc: 67.57%.\n",
      "--- epoch: 41 ---\n",
      "training loss: 0.4074, acc: 85.98%, dev loss: 0.8502, acc: 65.54%.\n",
      "--- epoch: 42 ---\n",
      "training loss: 0.4234, acc: 83.61%, dev loss: 0.8817, acc: 66.89%.\n",
      "--- epoch: 43 ---\n",
      "training loss: 0.4324, acc: 84.29%, dev loss: 0.8265, acc: 73.65%.\n",
      "--- epoch: 44 ---\n",
      "training loss: 0.3853, acc: 85.98%, dev loss: 0.7851, acc: 71.62%.\n",
      "--- epoch: 45 ---\n",
      "training loss: 0.4045, acc: 85.47%, dev loss: 0.7881, acc: 72.30%.\n",
      "--- epoch: 46 ---\n",
      "training loss: 0.3496, acc: 87.33%, dev loss: 0.8571, acc: 69.59%.\n",
      "--- epoch: 47 ---\n",
      "training loss: 0.3374, acc: 87.33%, dev loss: 0.8100, acc: 74.32%.\n",
      "--- epoch: 48 ---\n",
      "training loss: 0.3233, acc: 89.02%, dev loss: 0.8504, acc: 70.27%.\n",
      "--- epoch: 49 ---\n",
      "training loss: 0.3170, acc: 88.18%, dev loss: 0.9048, acc: 70.27%.\n",
      "--- epoch: 50 ---\n",
      "training loss: 0.3009, acc: 89.36%, dev loss: 0.8242, acc: 70.27%.\n",
      "--- epoch: 51 ---\n",
      "training loss: 0.2970, acc: 89.86%, dev loss: 0.8617, acc: 75.00%.\n",
      "--- epoch: 52 ---\n",
      "training loss: 0.3273, acc: 89.70%, dev loss: 0.8000, acc: 73.65%.\n",
      "--- epoch: 53 ---\n",
      "training loss: 0.2919, acc: 89.02%, dev loss: 0.9095, acc: 68.24%.\n",
      "--- epoch: 54 ---\n",
      "training loss: 0.2578, acc: 90.37%, dev loss: 0.8820, acc: 71.62%.\n",
      "--- epoch: 55 ---\n",
      "training loss: 0.3268, acc: 88.34%, dev loss: 0.7956, acc: 71.62%.\n",
      "--- epoch: 56 ---\n",
      "training loss: 0.3371, acc: 88.18%, dev loss: 0.8053, acc: 72.30%.\n",
      "--- epoch: 57 ---\n",
      "training loss: 0.3285, acc: 89.70%, dev loss: 0.7615, acc: 73.65%.\n",
      "--- epoch: 58 ---\n",
      "training loss: 0.2481, acc: 91.72%, dev loss: 0.8372, acc: 73.65%.\n",
      "--- epoch: 59 ---\n",
      "training loss: 0.2485, acc: 91.22%, dev loss: 0.8962, acc: 71.62%.\n",
      "--- epoch: 60 ---\n",
      "training loss: 0.2460, acc: 92.06%, dev loss: 0.8920, acc: 72.30%.\n",
      "--- epoch: 61 ---\n",
      "training loss: 0.3006, acc: 88.18%, dev loss: 0.7541, acc: 75.00%.\n",
      "--- epoch: 62 ---\n",
      "training loss: 0.2894, acc: 90.20%, dev loss: 0.8133, acc: 72.97%.\n",
      "--- epoch: 63 ---\n",
      "training loss: 0.2870, acc: 90.37%, dev loss: 0.7970, acc: 71.62%.\n",
      "--- epoch: 64 ---\n",
      "training loss: 0.2662, acc: 92.23%, dev loss: 0.8410, acc: 72.30%.\n",
      "--- epoch: 65 ---\n",
      "training loss: 0.2773, acc: 90.03%, dev loss: 0.8397, acc: 75.00%.\n",
      "--- epoch: 66 ---\n",
      "training loss: 0.2357, acc: 92.91%, dev loss: 0.9119, acc: 68.24%.\n",
      "--- epoch: 67 ---\n",
      "training loss: 0.2355, acc: 91.55%, dev loss: 0.8394, acc: 71.62%.\n",
      "--- epoch: 68 ---\n",
      "training loss: 0.3012, acc: 90.20%, dev loss: 0.8641, acc: 71.62%.\n",
      "--- epoch: 69 ---\n",
      "training loss: 0.2403, acc: 91.55%, dev loss: 0.8802, acc: 72.30%.\n",
      "--- epoch: 70 ---\n",
      "training loss: 0.2775, acc: 90.88%, dev loss: 0.8810, acc: 68.24%.\n",
      "--- epoch: 71 ---\n",
      "training loss: 0.2647, acc: 90.71%, dev loss: 0.8796, acc: 70.27%.\n",
      "--- epoch: 72 ---\n",
      "training loss: 0.2250, acc: 93.24%, dev loss: 0.9237, acc: 72.97%.\n",
      "--- epoch: 73 ---\n",
      "training loss: 0.2108, acc: 92.74%, dev loss: 0.8302, acc: 73.65%.\n",
      "--- epoch: 74 ---\n",
      "training loss: 0.1866, acc: 93.92%, dev loss: 0.9804, acc: 70.27%.\n",
      "--- epoch: 75 ---\n",
      "training loss: 0.2431, acc: 90.37%, dev loss: 0.8437, acc: 72.30%.\n",
      "--- epoch: 76 ---\n",
      "training loss: 0.2156, acc: 93.24%, dev loss: 0.8632, acc: 70.95%.\n",
      "--- epoch: 77 ---\n",
      "training loss: 0.2453, acc: 91.22%, dev loss: 1.0047, acc: 68.92%.\n",
      "--- epoch: 78 ---\n",
      "training loss: 0.2147, acc: 92.23%, dev loss: 0.8105, acc: 69.59%.\n",
      "--- epoch: 79 ---\n",
      "training loss: 0.2293, acc: 92.23%, dev loss: 0.9026, acc: 72.97%.\n",
      "--- epoch: 80 ---\n",
      "training loss: 0.2341, acc: 91.89%, dev loss: 0.7841, acc: 72.30%.\n",
      "--- epoch: 81 ---\n",
      "training loss: 0.2348, acc: 92.91%, dev loss: 0.7917, acc: 71.62%.\n",
      "--- epoch: 82 ---\n",
      "training loss: 0.2229, acc: 92.57%, dev loss: 0.8751, acc: 73.65%.\n",
      "--- epoch: 83 ---\n",
      "training loss: 0.1918, acc: 93.24%, dev loss: 0.9859, acc: 73.65%.\n",
      "--- epoch: 84 ---\n",
      "training loss: 0.2186, acc: 91.89%, dev loss: 0.8814, acc: 74.32%.\n",
      "--- epoch: 85 ---\n",
      "training loss: 0.2229, acc: 93.58%, dev loss: 0.8861, acc: 74.32%.\n",
      "--- epoch: 86 ---\n",
      "training loss: 0.1938, acc: 94.26%, dev loss: 0.8388, acc: 72.30%.\n",
      "--- epoch: 87 ---\n",
      "training loss: 0.2107, acc: 93.07%, dev loss: 0.8481, acc: 73.65%.\n",
      "--- epoch: 88 ---\n",
      "training loss: 0.1945, acc: 93.58%, dev loss: 0.9135, acc: 68.24%.\n",
      "--- epoch: 89 ---\n",
      "training loss: 0.1820, acc: 93.92%, dev loss: 0.8765, acc: 75.00%.\n",
      "--- epoch: 90 ---\n",
      "training loss: 0.2437, acc: 91.39%, dev loss: 0.8852, acc: 72.30%.\n",
      "--- epoch: 91 ---\n",
      "training loss: 0.2075, acc: 94.09%, dev loss: 0.8260, acc: 71.62%.\n",
      "--- epoch: 92 ---\n",
      "training loss: 0.1672, acc: 95.44%, dev loss: 0.9758, acc: 70.27%.\n",
      "--- epoch: 93 ---\n",
      "training loss: 0.2039, acc: 94.93%, dev loss: 0.8678, acc: 70.95%.\n",
      "--- epoch: 94 ---\n",
      "training loss: 0.2420, acc: 92.23%, dev loss: 0.9952, acc: 72.30%.\n",
      "--- epoch: 95 ---\n",
      "training loss: 0.2093, acc: 93.07%, dev loss: 0.8301, acc: 75.00%.\n",
      "--- epoch: 96 ---\n",
      "training loss: 0.1830, acc: 94.26%, dev loss: 1.0356, acc: 74.32%.\n",
      "--- epoch: 97 ---\n",
      "training loss: 0.1896, acc: 93.58%, dev loss: 0.9035, acc: 70.27%.\n",
      "--- epoch: 98 ---\n",
      "training loss: 0.1761, acc: 94.43%, dev loss: 0.8635, acc: 75.00%.\n",
      "--- epoch: 99 ---\n",
      "training loss: 0.2019, acc: 92.91%, dev loss: 0.9367, acc: 71.62%.\n",
      "--- epoch: 100 ---\n",
      "training loss: 0.1370, acc: 96.28%, dev loss: 1.1025, acc: 71.62%.\n",
      "best dev acc: 0.7500\n",
      "===========fold:5==============\n",
      "--- epoch: 1 ---\n",
      "training loss: 1.6443, acc: 29.05%, dev loss: 1.4208, acc: 40.54%.\n",
      "--- epoch: 2 ---\n",
      "training loss: 1.4463, acc: 40.54%, dev loss: 1.3374, acc: 43.24%.\n",
      "--- epoch: 3 ---\n",
      "training loss: 1.3664, acc: 45.61%, dev loss: 1.2579, acc: 46.62%.\n",
      "--- epoch: 4 ---\n",
      "training loss: 1.2812, acc: 46.96%, dev loss: 1.2607, acc: 47.30%.\n",
      "--- epoch: 5 ---\n",
      "training loss: 1.2514, acc: 51.01%, dev loss: 1.1461, acc: 51.35%.\n",
      "--- epoch: 6 ---\n",
      "training loss: 1.1213, acc: 58.45%, dev loss: 1.0223, acc: 58.11%.\n",
      "--- epoch: 7 ---\n",
      "training loss: 1.0459, acc: 61.32%, dev loss: 0.9611, acc: 60.14%.\n",
      "--- epoch: 8 ---\n",
      "training loss: 0.9634, acc: 66.89%, dev loss: 0.9986, acc: 60.81%.\n",
      "--- epoch: 9 ---\n",
      "training loss: 0.9112, acc: 65.88%, dev loss: 0.9118, acc: 60.14%.\n",
      "--- epoch: 10 ---\n",
      "training loss: 0.8752, acc: 67.40%, dev loss: 0.9172, acc: 62.84%.\n",
      "--- epoch: 11 ---\n",
      "training loss: 0.8435, acc: 69.93%, dev loss: 0.8503, acc: 64.19%.\n",
      "--- epoch: 12 ---\n",
      "training loss: 0.7931, acc: 71.79%, dev loss: 0.8960, acc: 66.22%.\n",
      "--- epoch: 13 ---\n",
      "training loss: 0.7542, acc: 73.14%, dev loss: 0.8385, acc: 68.24%.\n",
      "--- epoch: 14 ---\n",
      "training loss: 0.7245, acc: 73.82%, dev loss: 0.7913, acc: 70.95%.\n",
      "--- epoch: 15 ---\n",
      "training loss: 0.6937, acc: 75.34%, dev loss: 0.8655, acc: 68.92%.\n",
      "--- epoch: 16 ---\n",
      "training loss: 0.7261, acc: 73.65%, dev loss: 0.8424, acc: 70.27%.\n",
      "--- epoch: 17 ---\n",
      "training loss: 0.6738, acc: 74.83%, dev loss: 0.8109, acc: 64.86%.\n",
      "--- epoch: 18 ---\n",
      "training loss: 0.6794, acc: 75.68%, dev loss: 0.9342, acc: 68.24%.\n",
      "--- epoch: 19 ---\n",
      "training loss: 0.6663, acc: 75.84%, dev loss: 0.7213, acc: 70.27%.\n",
      "--- epoch: 20 ---\n",
      "training loss: 0.6496, acc: 77.53%, dev loss: 0.8799, acc: 64.19%.\n",
      "--- epoch: 21 ---\n",
      "training loss: 0.6209, acc: 79.22%, dev loss: 0.7821, acc: 70.27%.\n",
      "--- epoch: 22 ---\n",
      "training loss: 0.6071, acc: 76.86%, dev loss: 0.7406, acc: 72.30%.\n",
      "--- epoch: 23 ---\n",
      "training loss: 0.5065, acc: 81.08%, dev loss: 0.6991, acc: 71.62%.\n",
      "--- epoch: 24 ---\n",
      "training loss: 0.5325, acc: 81.76%, dev loss: 0.7343, acc: 71.62%.\n",
      "--- epoch: 25 ---\n",
      "training loss: 0.5042, acc: 80.74%, dev loss: 0.7427, acc: 75.00%.\n",
      "--- epoch: 26 ---\n",
      "training loss: 0.4762, acc: 81.93%, dev loss: 0.7707, acc: 69.59%.\n",
      "--- epoch: 27 ---\n",
      "training loss: 0.4536, acc: 84.46%, dev loss: 0.7290, acc: 74.32%.\n",
      "--- epoch: 28 ---\n",
      "training loss: 0.4991, acc: 82.94%, dev loss: 0.6912, acc: 71.62%.\n",
      "--- epoch: 29 ---\n",
      "training loss: 0.4978, acc: 83.61%, dev loss: 0.6684, acc: 78.38%.\n",
      "--- epoch: 30 ---\n",
      "training loss: 0.4340, acc: 83.95%, dev loss: 0.7252, acc: 70.95%.\n",
      "--- epoch: 31 ---\n",
      "training loss: 0.4188, acc: 86.32%, dev loss: 0.6753, acc: 73.65%.\n",
      "--- epoch: 32 ---\n",
      "training loss: 0.4355, acc: 83.45%, dev loss: 0.6883, acc: 74.32%.\n",
      "--- epoch: 33 ---\n",
      "training loss: 0.3388, acc: 88.68%, dev loss: 0.6760, acc: 72.97%.\n",
      "--- epoch: 34 ---\n",
      "training loss: 0.3446, acc: 88.68%, dev loss: 0.7158, acc: 71.62%.\n",
      "--- epoch: 35 ---\n",
      "training loss: 0.4070, acc: 85.98%, dev loss: 0.7699, acc: 75.68%.\n",
      "--- epoch: 36 ---\n",
      "training loss: 0.3547, acc: 88.34%, dev loss: 0.8021, acc: 72.30%.\n",
      "--- epoch: 37 ---\n",
      "training loss: 0.3645, acc: 88.01%, dev loss: 0.8060, acc: 73.65%.\n",
      "--- epoch: 38 ---\n",
      "training loss: 0.3127, acc: 88.18%, dev loss: 0.7149, acc: 73.65%.\n",
      "--- epoch: 39 ---\n",
      "training loss: 0.3253, acc: 88.51%, dev loss: 0.7117, acc: 73.65%.\n",
      "--- epoch: 40 ---\n",
      "training loss: 0.2919, acc: 89.53%, dev loss: 0.7891, acc: 69.59%.\n",
      "--- epoch: 41 ---\n",
      "training loss: 0.3163, acc: 89.19%, dev loss: 0.6697, acc: 72.97%.\n",
      "--- epoch: 42 ---\n",
      "training loss: 0.2924, acc: 89.86%, dev loss: 0.7006, acc: 73.65%.\n",
      "--- epoch: 43 ---\n",
      "training loss: 0.2718, acc: 91.22%, dev loss: 0.7442, acc: 75.00%.\n",
      "--- epoch: 44 ---\n",
      "training loss: 0.2353, acc: 91.22%, dev loss: 0.7972, acc: 68.92%.\n",
      "--- epoch: 45 ---\n",
      "training loss: 0.2691, acc: 90.20%, dev loss: 0.7509, acc: 70.95%.\n",
      "--- epoch: 46 ---\n",
      "training loss: 0.2453, acc: 91.55%, dev loss: 0.6388, acc: 72.97%.\n",
      "--- epoch: 47 ---\n",
      "training loss: 0.2400, acc: 91.89%, dev loss: 0.6648, acc: 76.35%.\n",
      "--- epoch: 48 ---\n",
      "training loss: 0.2632, acc: 90.20%, dev loss: 0.7249, acc: 70.95%.\n",
      "--- epoch: 49 ---\n",
      "training loss: 0.2776, acc: 90.71%, dev loss: 0.6727, acc: 75.00%.\n",
      "--- epoch: 50 ---\n",
      "training loss: 0.2532, acc: 91.22%, dev loss: 0.6651, acc: 75.68%.\n",
      "--- epoch: 51 ---\n",
      "training loss: 0.2799, acc: 89.36%, dev loss: 0.7780, acc: 74.32%.\n",
      "--- epoch: 52 ---\n",
      "training loss: 0.2576, acc: 90.71%, dev loss: 0.6481, acc: 74.32%.\n",
      "--- epoch: 53 ---\n",
      "training loss: 0.2544, acc: 91.72%, dev loss: 0.7222, acc: 75.00%.\n",
      "--- epoch: 54 ---\n",
      "training loss: 0.2892, acc: 89.86%, dev loss: 0.6386, acc: 75.00%.\n",
      "--- epoch: 55 ---\n",
      "training loss: 0.2788, acc: 91.39%, dev loss: 0.6284, acc: 77.70%.\n",
      "--- epoch: 56 ---\n",
      "training loss: 0.2058, acc: 95.27%, dev loss: 0.7051, acc: 71.62%.\n",
      "--- epoch: 57 ---\n",
      "training loss: 0.2436, acc: 92.57%, dev loss: 0.8120, acc: 72.30%.\n",
      "--- epoch: 58 ---\n",
      "training loss: 0.2335, acc: 92.91%, dev loss: 0.8104, acc: 72.97%.\n",
      "--- epoch: 59 ---\n",
      "training loss: 0.2173, acc: 91.39%, dev loss: 0.7453, acc: 72.97%.\n",
      "--- epoch: 60 ---\n",
      "training loss: 0.2202, acc: 93.07%, dev loss: 0.6974, acc: 75.68%.\n",
      "--- epoch: 61 ---\n",
      "training loss: 0.1956, acc: 94.43%, dev loss: 0.7353, acc: 72.30%.\n",
      "--- epoch: 62 ---\n",
      "training loss: 0.1840, acc: 93.92%, dev loss: 0.7266, acc: 70.95%.\n",
      "--- epoch: 63 ---\n",
      "training loss: 0.1598, acc: 95.27%, dev loss: 0.7644, acc: 73.65%.\n",
      "--- epoch: 64 ---\n",
      "training loss: 0.1753, acc: 94.76%, dev loss: 0.7200, acc: 71.62%.\n",
      "--- epoch: 65 ---\n",
      "training loss: 0.1915, acc: 93.41%, dev loss: 0.7064, acc: 75.00%.\n",
      "--- epoch: 66 ---\n",
      "training loss: 0.1768, acc: 95.10%, dev loss: 0.7826, acc: 73.65%.\n",
      "--- epoch: 67 ---\n",
      "training loss: 0.1713, acc: 94.76%, dev loss: 0.7753, acc: 74.32%.\n",
      "--- epoch: 68 ---\n",
      "training loss: 0.2051, acc: 92.91%, dev loss: 0.7204, acc: 75.00%.\n",
      "--- epoch: 69 ---\n",
      "training loss: 0.2089, acc: 92.57%, dev loss: 0.7217, acc: 71.62%.\n",
      "--- epoch: 70 ---\n",
      "training loss: 0.1978, acc: 94.76%, dev loss: 0.8866, acc: 72.30%.\n",
      "--- epoch: 71 ---\n",
      "training loss: 0.2087, acc: 92.40%, dev loss: 0.6464, acc: 76.35%.\n",
      "--- epoch: 72 ---\n",
      "training loss: 0.2041, acc: 94.09%, dev loss: 0.6991, acc: 77.03%.\n",
      "--- epoch: 73 ---\n",
      "training loss: 0.1992, acc: 93.58%, dev loss: 0.8113, acc: 70.95%.\n",
      "--- epoch: 74 ---\n",
      "training loss: 0.1794, acc: 94.09%, dev loss: 0.7395, acc: 75.00%.\n",
      "--- epoch: 75 ---\n",
      "training loss: 0.2042, acc: 92.74%, dev loss: 0.6831, acc: 76.35%.\n",
      "--- epoch: 76 ---\n",
      "training loss: 0.2364, acc: 92.06%, dev loss: 0.8358, acc: 71.62%.\n",
      "--- epoch: 77 ---\n",
      "training loss: 0.2286, acc: 92.91%, dev loss: 0.6498, acc: 77.70%.\n",
      "--- epoch: 78 ---\n",
      "training loss: 0.1493, acc: 96.28%, dev loss: 0.6824, acc: 71.62%.\n",
      "--- epoch: 79 ---\n",
      "training loss: 0.1619, acc: 95.10%, dev loss: 0.7149, acc: 72.97%.\n",
      "--- epoch: 80 ---\n",
      "training loss: 0.1858, acc: 94.43%, dev loss: 0.7227, acc: 72.97%.\n",
      "--- epoch: 81 ---\n",
      "training loss: 0.1674, acc: 95.10%, dev loss: 0.7283, acc: 72.30%.\n",
      "--- epoch: 82 ---\n",
      "training loss: 0.1530, acc: 94.43%, dev loss: 0.8117, acc: 72.97%.\n",
      "--- epoch: 83 ---\n",
      "training loss: 0.1542, acc: 94.76%, dev loss: 0.7787, acc: 74.32%.\n",
      "--- epoch: 84 ---\n",
      "training loss: 0.1771, acc: 93.41%, dev loss: 0.7287, acc: 75.68%.\n",
      "--- epoch: 85 ---\n",
      "training loss: 0.1839, acc: 93.75%, dev loss: 0.8214, acc: 70.27%.\n",
      "--- epoch: 86 ---\n",
      "training loss: 0.1961, acc: 92.91%, dev loss: 0.7369, acc: 69.59%.\n",
      "--- epoch: 87 ---\n",
      "training loss: 0.2125, acc: 91.89%, dev loss: 0.7976, acc: 72.30%.\n",
      "--- epoch: 88 ---\n",
      "training loss: 0.2272, acc: 91.55%, dev loss: 0.7143, acc: 72.97%.\n",
      "--- epoch: 89 ---\n",
      "training loss: 0.2058, acc: 93.24%, dev loss: 0.7132, acc: 75.00%.\n",
      "--- epoch: 90 ---\n",
      "training loss: 0.1448, acc: 96.79%, dev loss: 0.6530, acc: 75.68%.\n",
      "--- epoch: 91 ---\n",
      "training loss: 0.1490, acc: 94.26%, dev loss: 0.7489, acc: 75.68%.\n",
      "--- epoch: 92 ---\n",
      "training loss: 0.1394, acc: 95.44%, dev loss: 0.6961, acc: 79.05%.\n",
      "--- epoch: 93 ---\n",
      "training loss: 0.1237, acc: 96.28%, dev loss: 0.7309, acc: 75.68%.\n",
      "--- epoch: 94 ---\n",
      "training loss: 0.1586, acc: 95.10%, dev loss: 0.6412, acc: 75.00%.\n",
      "--- epoch: 95 ---\n",
      "training loss: 0.1719, acc: 94.76%, dev loss: 0.6909, acc: 73.65%.\n",
      "--- epoch: 96 ---\n",
      "training loss: 0.1394, acc: 95.78%, dev loss: 0.8601, acc: 70.27%.\n",
      "--- epoch: 97 ---\n",
      "training loss: 0.1790, acc: 94.59%, dev loss: 0.7710, acc: 77.03%.\n",
      "--- epoch: 98 ---\n",
      "training loss: 0.1704, acc: 94.59%, dev loss: 0.6817, acc: 74.32%.\n",
      "--- epoch: 99 ---\n",
      "training loss: 0.1561, acc: 96.28%, dev loss: 0.6476, acc: 75.00%.\n",
      "--- epoch: 100 ---\n",
      "training loss: 0.1532, acc: 94.76%, dev loss: 0.7037, acc: 72.97%.\n",
      "best dev acc: 0.7905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "K = 5\n",
    "skf = StratifiedKFold(n_splits=K, random_state=seed, shuffle=True)\n",
    "models = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(data, label)):\n",
    "    print(f'===========fold:{fold+1}==============')\n",
    "    train_data, train_label = data[train_idx], label[train_idx]\n",
    "    val_data, val_label = data[val_idx], label[val_idx]\n",
    "    train_set = FlowerDataset(train_data, train_label)\n",
    "    dev_set = FlowerDataset(val_data, val_label)\n",
    "\n",
    "    train_data_loader = DataLoader(train_set, 32, True)\n",
    "    dev_data_loader = DataLoader(dev_set, 32, False)\n",
    "\n",
    "    net = MyModel()\n",
    "    best_net = train(net, train_data_loader, dev_data_loader)\n",
    "    model_path = f'C:\\\\Users\\zzzgry\\Desktop\\midspore_lab2\\model\\\\pytorch\\\\fold{fold+1}-model-'+time.strftime('%m-%d_%H.%M', time.localtime())+'.pkl'\n",
    "    torch.save(best_net, model_path)\n",
    "    net.load_state_dict(torch.load(model_path))\n",
    "    models.append(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data_loader):\n",
    "    test_stat = Stat(training=False)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader:\n",
    "            data, labels = batch[0], batch[1]\n",
    "            pred_outputs = model(data)\n",
    "            test_stat.add(pred_outputs, labels, 0)\n",
    "    return test_stat.pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 3, 100, 100]), torch.Size([6]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_test = '../TestImages/'\n",
    "imgs=[]\n",
    "for im in glob.glob(path_test+'/*.jpg'):\n",
    "    img=cv2.imread(im)           \n",
    "    img=cv2.resize(img,(w,h)) \n",
    "    imgs.append(img)  \n",
    "imgs = np.asarray(imgs,np.float32)    \n",
    "\n",
    "test_data = imgs / 255\n",
    "test_label = [x for x in range(len(test_data))]\n",
    "\n",
    "test_data = torch.FloatTensor(test_data).permute(0, 3, 1, 2)\n",
    "test_label = torch.LongTensor(test_label)\n",
    "test_data.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = FlowerDataset(test_data, test_label)\n",
    "test_data_loader = DataLoader(test_set, batch_size=len(test_data), shuffle=False)\n",
    "# test(net, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 1.0\n"
     ]
    }
   ],
   "source": [
    "pred_labels = []\n",
    "for net in models:\n",
    "    pred_label = test(net, test_data_loader)\n",
    "    pred_labels.append(pred_label)\n",
    "pred_labels = np.array(pred_labels).T\n",
    "# 这样pred_labels的行向量为第i个样本的预测值\n",
    "final_pred = []\n",
    "for sample in pred_labels:\n",
    "    final_pred.append(np.argmax(np.bincount(sample)))\n",
    "\n",
    "correct = [1 if pred_label==idx else 0 for idx, pred_label in enumerate(final_pred)]\n",
    "print(f'acc = {sum(correct)/len(correct)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.9932432432432432\n"
     ]
    }
   ],
   "source": [
    "# pred_labels = []\n",
    "# for net in models:\n",
    "#     pred_label = test(net, dev_data_loader)\n",
    "#     pred_labels.append(pred_label)\n",
    "# pred_labels = np.array(pred_labels).T\n",
    "# # 这样pred_labels的行向量为第i个样本的预测值\n",
    "# final_pred = []\n",
    "# for sample in pred_labels:\n",
    "#     final_pred.append(np.argmax(np.bincount(sample)))\n",
    "\n",
    "# correct = [1 if pred_label==val_label[idx] else 0 for idx, pred_label in enumerate(final_pred)]\n",
    "# print(f'acc = {sum(correct)/len(correct)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = MyModel()\n",
    "# best_net = train(net, train_data_loader, dev_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(best_net, model_path)\n",
    "# net.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, test_data_loader):\n",
    "#     test_stat = Stat(training=False)\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_data_loader:\n",
    "#             data, labels = batch[0], batch[1]\n",
    "#             pred_outputs = model(data)\n",
    "#             test_stat.add(pred_outputs, labels, 0)\n",
    "\n",
    "#     report = classification_report(\n",
    "#         test_stat.labels,\n",
    "#         test_stat.pred_labels,\n",
    "#         target_names=class_list,\n",
    "#         digits=4,\n",
    "#         zero_division=0,\n",
    "#     )\n",
    "#     print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_test = '../TestImages/'\n",
    "# imgs=[]\n",
    "# for im in glob.glob(path_test+'/*.jpg'):\n",
    "#     img=cv2.imread(im)           \n",
    "#     img=cv2.resize(img,(w,h)) \n",
    "#     imgs.append(img)  \n",
    "# imgs = np.asarray(imgs,np.float32)    \n",
    "\n",
    "# test_data = imgs / 255\n",
    "# test_label = [x for x in range(len(test_data))]\n",
    "\n",
    "# test_data = torch.FloatTensor(test_data).permute(0, 3, 1, 2)\n",
    "# test_label = torch.LongTensor(test_label)\n",
    "# test_data.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = FlowerDataset(test_data, test_label)\n",
    "# test_data_loader = DataLoader(test_set, batch_size=len(test_data), shuffle=False)\n",
    "# test(net, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = MyModel()\n",
    "# net.load_state_dict(torch.load(model_path))\n",
    "# test(net, test_data_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0a0aef9578cc1edfecec80d6a58c391f944f9223b5cad1c1ac9c6fdf88bcc20"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('basework')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
