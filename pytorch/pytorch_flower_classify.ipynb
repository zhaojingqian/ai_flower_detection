{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../flower_photos/'\n",
    "w = 100\n",
    "h = 100\n",
    "c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    cate=[path+x for x in os.listdir(path) if os.path.isdir(path+x)] \n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for idx, folder in enumerate(cate):\n",
    "        for im in glob.glob(folder + '/*.jpg'):\n",
    "            img = cv2.imread(im)\n",
    "            img = cv2.resize(img, (w, h))\n",
    "            imgs.append(img)\n",
    "            labels.append(idx)\n",
    "    return np.asarray(imgs, np.float32), np.asarray(labels, np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([740, 3, 100, 100]), torch.Size([740]))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = read_img(path)\n",
    "\n",
    "data = torch.FloatTensor(data).permute(0, 3, 1, 2)\n",
    "label = torch.LongTensor(label)\n",
    "\n",
    "data.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 109\n",
    "np.random.seed(seed)\n",
    "\n",
    "data = data / 255\n",
    "(x_train, x_val, y_train, y_val) = train_test_split(data, label, test_size=0.20, random_state=seed)\n",
    "# x_train = x_train / 255\n",
    "# x_val = x_val / 255\n",
    "\n",
    "flower_dict = {0:'bee', 1:'blackberry', 2:'blanket', 3:'bougainvilliea', 4:'bromelia', 5:'foxglove'}\n",
    "class_list = ['bee', 'blackberry', 'blanket', 'bougainvilliea', 'bromelia', 'foxglove']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TORCH框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        super(FlowerDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    model\n",
    "    采取和tf一样的CNN架构\n",
    "'''\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class SelfAttention(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(SelfAttention, self).__init__()\n",
    "#         self.projection = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 64),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(64, 1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, data):\n",
    "#         energy = self.projection(data)\n",
    "#         weights = F.softmax(energy, dim=1)\n",
    "#         outputs = data * weights\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.conv_model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        # self.attention = SelfAttention(12*12)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(12*12*64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 6),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.conv_model(data)\n",
    "        x = x.view(x.size()[0],-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stat:\n",
    "    def __init__(self, training, writer=None):\n",
    "        self.step = 0\n",
    "        self.loss = []\n",
    "        self.labels = []\n",
    "        self.pred_labels = []\n",
    "        self.training = training\n",
    "        self.writer = writer\n",
    "    \n",
    "    def add(self, pred, labels, loss):\n",
    "        labels = labels.numpy()\n",
    "        pred = pred.detach().numpy()\n",
    "        pred_labels = np.argmax(pred, axis = 1)\n",
    "        self.loss.append(loss)\n",
    "        self.labels.extend(labels)\n",
    "        self.pred_labels.extend(pred_labels)\n",
    "\n",
    "    def log(self):\n",
    "        self.step += 1\n",
    "        acc = accuracy_score(self.labels, self.pred_labels)\n",
    "        loss = sum(self.loss) / len(self.loss)\n",
    "        self.loss = []\n",
    "        self.labels = []\n",
    "        self.pred_labels = []\n",
    "        if not self.writer:\n",
    "            return loss, acc\n",
    "        if self.training:\n",
    "            self.writer.add_scalar('train_loss', loss, self.step)\n",
    "            self.writer.add_scalar('train_acc', acc, self.step)\n",
    "        else:\n",
    "            self.writer.add_scalar('dev_loss', loss, self.step)\n",
    "            self.writer.add_scalar('dev_acc', acc, self.step)\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, dev_data_loader):\n",
    "    loss_func = CrossEntropyLoss()\n",
    "    optimizer = Adam(\n",
    "        model.parameters(),\n",
    "        lr=0.001,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    writer = SummaryWriter('./summary/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n",
    "    train_stat = Stat(training=True, writer=writer)\n",
    "    dev_stat = Stat(training=False, writer=writer)\n",
    "\n",
    "\n",
    "    best_acc, best_net = 0.0, None\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"--- epoch: {epoch + 1} ---\")\n",
    "        for iter, batch in enumerate(train_data_loader):\n",
    "            model.train()\n",
    "            data, labels = batch[0], batch[1]\n",
    "            pred_outputs = model(data)\n",
    "            loss = loss_func(pred_outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_stat.add(pred_outputs, labels, loss.item())\n",
    "            \n",
    "        train_loss, train_acc = train_stat.log()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in dev_data_loader:\n",
    "                data, labels = batch[0], batch[1]\n",
    "                pred_outputs = model(data)\n",
    "                loss = loss_func(pred_outputs, labels)\n",
    "                dev_stat.add(pred_outputs, labels, loss.item())\n",
    "        dev_loss, dev_acc = dev_stat.log()\n",
    "        print(  f\"training loss: {train_loss:.4f}, acc: {train_acc:.2%}, \" \\\n",
    "                f\"dev loss: {dev_loss:.4f}, acc: {dev_acc:.2%}.\")\n",
    "\n",
    "        if dev_acc > best_acc:\n",
    "            best_acc = dev_acc\n",
    "            best_net = deepcopy(model.state_dict())\n",
    "            \n",
    "    print(f\"best dev acc: {best_acc:.4f}\")\n",
    "    return best_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'C:\\\\Users\\zzzgry\\Desktop\\midspore_lab2\\model\\\\pytorch\\\\model-'+time.strftime('%m-%d_%H.%M', time.localtime())+'.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FlowerDataset(x_train, y_train)\n",
    "dev_set = FlowerDataset(x_val, y_val)\n",
    "\n",
    "train_data_loader = DataLoader(train_set, 32, True)\n",
    "dev_data_loader = DataLoader(dev_set, 32, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========fold:1==============\n",
      "--- epoch: 1 ---\n",
      "training loss: 1.7724, acc: 27.20%, dev loss: 1.5070, acc: 30.41%.\n",
      "--- epoch: 2 ---\n",
      "training loss: 1.4542, acc: 40.03%, dev loss: 1.4356, acc: 40.54%.\n",
      "--- epoch: 3 ---\n",
      "training loss: 1.5051, acc: 40.71%, dev loss: 1.2728, acc: 45.27%.\n",
      "--- epoch: 4 ---\n",
      "training loss: 1.3932, acc: 43.24%, dev loss: 1.2719, acc: 49.32%.\n",
      "--- epoch: 5 ---\n",
      "training loss: 1.3401, acc: 47.47%, dev loss: 1.2398, acc: 48.65%.\n",
      "--- epoch: 6 ---\n",
      "training loss: 1.2795, acc: 48.31%, dev loss: 1.1748, acc: 51.35%.\n",
      "--- epoch: 7 ---\n",
      "training loss: 1.2965, acc: 48.31%, dev loss: 1.1858, acc: 50.68%.\n",
      "--- epoch: 8 ---\n",
      "training loss: 1.2692, acc: 49.66%, dev loss: 1.1444, acc: 55.41%.\n",
      "--- epoch: 9 ---\n",
      "training loss: 1.2391, acc: 54.73%, dev loss: 1.2619, acc: 51.35%.\n",
      "--- epoch: 10 ---\n",
      "training loss: 1.2779, acc: 51.86%, dev loss: 1.3609, acc: 45.95%.\n",
      "--- epoch: 11 ---\n",
      "training loss: 1.2044, acc: 52.70%, dev loss: 1.0464, acc: 60.14%.\n",
      "--- epoch: 12 ---\n",
      "training loss: 1.1619, acc: 58.28%, dev loss: 1.1128, acc: 61.49%.\n",
      "--- epoch: 13 ---\n",
      "training loss: 1.0935, acc: 56.93%, dev loss: 1.1278, acc: 54.73%.\n",
      "--- epoch: 14 ---\n",
      "training loss: 1.0426, acc: 60.98%, dev loss: 0.9716, acc: 62.16%.\n",
      "--- epoch: 15 ---\n",
      "training loss: 1.0658, acc: 60.14%, dev loss: 1.0136, acc: 61.49%.\n",
      "--- epoch: 16 ---\n",
      "training loss: 0.9751, acc: 63.85%, dev loss: 1.0833, acc: 62.84%.\n",
      "--- epoch: 17 ---\n",
      "training loss: 1.0673, acc: 61.32%, dev loss: 0.9584, acc: 61.49%.\n",
      "--- epoch: 18 ---\n",
      "training loss: 0.9793, acc: 64.70%, dev loss: 1.0674, acc: 59.46%.\n",
      "--- epoch: 19 ---\n",
      "training loss: 0.9429, acc: 63.68%, dev loss: 0.9486, acc: 61.49%.\n",
      "--- epoch: 20 ---\n",
      "training loss: 0.9897, acc: 64.02%, dev loss: 0.9864, acc: 61.49%.\n",
      "--- epoch: 21 ---\n",
      "training loss: 0.9058, acc: 65.20%, dev loss: 0.9415, acc: 66.22%.\n",
      "--- epoch: 22 ---\n",
      "training loss: 0.9264, acc: 65.54%, dev loss: 0.9560, acc: 62.16%.\n",
      "--- epoch: 23 ---\n",
      "training loss: 0.9564, acc: 63.68%, dev loss: 1.0231, acc: 61.49%.\n",
      "--- epoch: 24 ---\n",
      "training loss: 0.8963, acc: 67.06%, dev loss: 0.9244, acc: 63.51%.\n",
      "--- epoch: 25 ---\n",
      "training loss: 0.8992, acc: 66.22%, dev loss: 0.9908, acc: 60.81%.\n",
      "--- epoch: 26 ---\n",
      "training loss: 0.9514, acc: 64.70%, dev loss: 1.0064, acc: 60.14%.\n",
      "--- epoch: 27 ---\n",
      "training loss: 0.9096, acc: 66.05%, dev loss: 0.9471, acc: 66.22%.\n",
      "--- epoch: 28 ---\n",
      "training loss: 0.8632, acc: 69.09%, dev loss: 0.9397, acc: 66.22%.\n",
      "--- epoch: 29 ---\n",
      "training loss: 0.9460, acc: 62.67%, dev loss: 1.0163, acc: 60.14%.\n",
      "--- epoch: 30 ---\n",
      "training loss: 0.9142, acc: 67.23%, dev loss: 0.9578, acc: 66.22%.\n",
      "--- epoch: 31 ---\n",
      "training loss: 0.8223, acc: 71.28%, dev loss: 0.9745, acc: 61.49%.\n",
      "--- epoch: 32 ---\n",
      "training loss: 0.8106, acc: 69.76%, dev loss: 0.9476, acc: 66.22%.\n",
      "--- epoch: 33 ---\n",
      "training loss: 0.8100, acc: 70.44%, dev loss: 0.8788, acc: 64.19%.\n",
      "--- epoch: 34 ---\n",
      "training loss: 0.8483, acc: 69.76%, dev loss: 1.0398, acc: 67.57%.\n",
      "--- epoch: 35 ---\n",
      "training loss: 0.7910, acc: 70.10%, dev loss: 0.8700, acc: 66.89%.\n",
      "--- epoch: 36 ---\n",
      "training loss: 0.7911, acc: 70.27%, dev loss: 0.9428, acc: 66.22%.\n",
      "--- epoch: 37 ---\n",
      "training loss: 0.7617, acc: 72.30%, dev loss: 0.9510, acc: 64.86%.\n",
      "--- epoch: 38 ---\n",
      "training loss: 0.7832, acc: 72.13%, dev loss: 0.8518, acc: 71.62%.\n",
      "--- epoch: 39 ---\n",
      "training loss: 0.7983, acc: 72.47%, dev loss: 0.8736, acc: 66.89%.\n",
      "--- epoch: 40 ---\n",
      "training loss: 0.7478, acc: 71.96%, dev loss: 1.2519, acc: 61.49%.\n",
      "--- epoch: 41 ---\n",
      "training loss: 0.9864, acc: 64.02%, dev loss: 1.1068, acc: 65.54%.\n",
      "--- epoch: 42 ---\n",
      "training loss: 0.9468, acc: 66.22%, dev loss: 1.0915, acc: 61.49%.\n",
      "--- epoch: 43 ---\n",
      "training loss: 0.8476, acc: 68.58%, dev loss: 0.9170, acc: 68.24%.\n",
      "--- epoch: 44 ---\n",
      "training loss: 0.8062, acc: 70.27%, dev loss: 0.8845, acc: 64.86%.\n",
      "--- epoch: 45 ---\n",
      "training loss: 0.7229, acc: 73.14%, dev loss: 0.9160, acc: 66.89%.\n",
      "--- epoch: 46 ---\n",
      "training loss: 0.7492, acc: 71.62%, dev loss: 0.9634, acc: 64.86%.\n",
      "--- epoch: 47 ---\n",
      "training loss: 0.7387, acc: 72.97%, dev loss: 0.8691, acc: 67.57%.\n",
      "--- epoch: 48 ---\n",
      "training loss: 0.7519, acc: 75.00%, dev loss: 0.8720, acc: 66.22%.\n",
      "--- epoch: 49 ---\n",
      "training loss: 0.7333, acc: 72.80%, dev loss: 0.9058, acc: 64.19%.\n",
      "--- epoch: 50 ---\n",
      "training loss: 0.7104, acc: 72.80%, dev loss: 1.0650, acc: 62.84%.\n",
      "--- epoch: 51 ---\n",
      "training loss: 0.7558, acc: 71.62%, dev loss: 0.8170, acc: 69.59%.\n",
      "--- epoch: 52 ---\n",
      "training loss: 0.6790, acc: 74.83%, dev loss: 0.8173, acc: 68.92%.\n",
      "--- epoch: 53 ---\n",
      "training loss: 0.6995, acc: 73.82%, dev loss: 0.8440, acc: 69.59%.\n",
      "--- epoch: 54 ---\n",
      "training loss: 0.7280, acc: 73.14%, dev loss: 0.8721, acc: 64.86%.\n",
      "--- epoch: 55 ---\n",
      "training loss: 0.6888, acc: 73.82%, dev loss: 0.8141, acc: 68.92%.\n",
      "--- epoch: 56 ---\n",
      "training loss: 0.6852, acc: 74.66%, dev loss: 0.8841, acc: 65.54%.\n",
      "--- epoch: 57 ---\n",
      "training loss: 0.6983, acc: 76.52%, dev loss: 0.9848, acc: 62.84%.\n",
      "--- epoch: 58 ---\n",
      "training loss: 0.7400, acc: 72.30%, dev loss: 0.8425, acc: 65.54%.\n",
      "--- epoch: 59 ---\n",
      "training loss: 0.6630, acc: 74.32%, dev loss: 0.8747, acc: 70.95%.\n",
      "--- epoch: 60 ---\n",
      "training loss: 0.7392, acc: 74.83%, dev loss: 0.8717, acc: 69.59%.\n",
      "--- epoch: 61 ---\n",
      "training loss: 0.7028, acc: 74.66%, dev loss: 0.8933, acc: 68.92%.\n",
      "--- epoch: 62 ---\n",
      "training loss: 0.7076, acc: 74.49%, dev loss: 0.8168, acc: 71.62%.\n",
      "--- epoch: 63 ---\n",
      "training loss: 0.6394, acc: 78.38%, dev loss: 0.8912, acc: 67.57%.\n",
      "--- epoch: 64 ---\n",
      "training loss: 0.6510, acc: 75.68%, dev loss: 0.8563, acc: 70.27%.\n",
      "--- epoch: 65 ---\n",
      "training loss: 0.6515, acc: 75.00%, dev loss: 0.8599, acc: 68.24%.\n",
      "--- epoch: 66 ---\n",
      "training loss: 0.6776, acc: 75.51%, dev loss: 0.9068, acc: 68.24%.\n",
      "--- epoch: 67 ---\n",
      "training loss: 0.6739, acc: 75.68%, dev loss: 0.9045, acc: 68.92%.\n",
      "--- epoch: 68 ---\n",
      "training loss: 0.6954, acc: 74.16%, dev loss: 0.8576, acc: 67.57%.\n",
      "--- epoch: 69 ---\n",
      "training loss: 0.6199, acc: 76.52%, dev loss: 0.8854, acc: 71.62%.\n",
      "--- epoch: 70 ---\n",
      "training loss: 0.5558, acc: 80.74%, dev loss: 0.8935, acc: 73.65%.\n",
      "--- epoch: 71 ---\n",
      "training loss: 0.7320, acc: 72.47%, dev loss: 0.8000, acc: 70.95%.\n",
      "--- epoch: 72 ---\n",
      "training loss: 0.6811, acc: 74.16%, dev loss: 0.9880, acc: 69.59%.\n",
      "--- epoch: 73 ---\n",
      "training loss: 0.6324, acc: 77.03%, dev loss: 1.0192, acc: 70.27%.\n",
      "--- epoch: 74 ---\n",
      "training loss: 0.5822, acc: 79.39%, dev loss: 0.7823, acc: 76.35%.\n",
      "--- epoch: 75 ---\n",
      "training loss: 0.6554, acc: 75.17%, dev loss: 0.8210, acc: 72.97%.\n",
      "--- epoch: 76 ---\n",
      "training loss: 0.6391, acc: 79.05%, dev loss: 0.8404, acc: 72.30%.\n",
      "--- epoch: 77 ---\n",
      "training loss: 0.6169, acc: 78.38%, dev loss: 0.8326, acc: 73.65%.\n",
      "--- epoch: 78 ---\n",
      "training loss: 0.6693, acc: 75.34%, dev loss: 0.9631, acc: 66.22%.\n",
      "--- epoch: 79 ---\n",
      "training loss: 0.6234, acc: 77.87%, dev loss: 0.8768, acc: 67.57%.\n",
      "--- epoch: 80 ---\n",
      "training loss: 0.6034, acc: 77.70%, dev loss: 0.8927, acc: 70.27%.\n",
      "--- epoch: 81 ---\n",
      "training loss: 0.5843, acc: 78.04%, dev loss: 0.9086, acc: 68.24%.\n",
      "--- epoch: 82 ---\n",
      "training loss: 0.5723, acc: 81.08%, dev loss: 0.8277, acc: 72.30%.\n",
      "--- epoch: 83 ---\n",
      "training loss: 0.6077, acc: 80.41%, dev loss: 0.8150, acc: 71.62%.\n",
      "--- epoch: 84 ---\n",
      "training loss: 0.7252, acc: 75.17%, dev loss: 0.8192, acc: 70.27%.\n",
      "--- epoch: 85 ---\n",
      "training loss: 0.6191, acc: 78.38%, dev loss: 1.0276, acc: 65.54%.\n",
      "--- epoch: 86 ---\n",
      "training loss: 0.5742, acc: 76.69%, dev loss: 0.8872, acc: 70.95%.\n",
      "--- epoch: 87 ---\n",
      "training loss: 0.6153, acc: 77.03%, dev loss: 0.9084, acc: 67.57%.\n",
      "--- epoch: 88 ---\n",
      "training loss: 0.5595, acc: 79.39%, dev loss: 0.7858, acc: 72.30%.\n",
      "--- epoch: 89 ---\n",
      "training loss: 0.5089, acc: 81.76%, dev loss: 0.7690, acc: 72.97%.\n",
      "--- epoch: 90 ---\n",
      "training loss: 0.4886, acc: 81.08%, dev loss: 0.8812, acc: 72.97%.\n",
      "--- epoch: 91 ---\n",
      "training loss: 0.5417, acc: 80.41%, dev loss: 0.8982, acc: 69.59%.\n",
      "--- epoch: 92 ---\n",
      "training loss: 0.5781, acc: 79.56%, dev loss: 0.8376, acc: 72.30%.\n",
      "--- epoch: 93 ---\n",
      "training loss: 0.6120, acc: 76.35%, dev loss: 1.1414, acc: 58.78%.\n",
      "--- epoch: 94 ---\n",
      "training loss: 0.5312, acc: 81.93%, dev loss: 0.8259, acc: 70.95%.\n",
      "--- epoch: 95 ---\n",
      "training loss: 0.6184, acc: 76.35%, dev loss: 0.8970, acc: 66.89%.\n",
      "--- epoch: 96 ---\n",
      "training loss: 0.6884, acc: 75.68%, dev loss: 0.7973, acc: 71.62%.\n",
      "--- epoch: 97 ---\n",
      "training loss: 0.6451, acc: 76.18%, dev loss: 0.9083, acc: 69.59%.\n",
      "--- epoch: 98 ---\n",
      "training loss: 0.5866, acc: 78.38%, dev loss: 0.7704, acc: 75.00%.\n",
      "--- epoch: 99 ---\n",
      "training loss: 0.5501, acc: 80.07%, dev loss: 0.9961, acc: 66.89%.\n",
      "--- epoch: 100 ---\n",
      "training loss: 0.5799, acc: 79.56%, dev loss: 0.8487, acc: 68.92%.\n",
      "best dev acc: 0.7635\n",
      "===========fold:2==============\n",
      "--- epoch: 1 ---\n",
      "training loss: 1.7804, acc: 27.03%, dev loss: 1.7010, acc: 29.73%.\n",
      "--- epoch: 2 ---\n",
      "training loss: 1.6630, acc: 29.56%, dev loss: 1.4448, acc: 43.24%.\n",
      "--- epoch: 3 ---\n",
      "training loss: 1.4375, acc: 41.05%, dev loss: 1.3041, acc: 45.27%.\n",
      "--- epoch: 4 ---\n",
      "training loss: 1.3836, acc: 44.59%, dev loss: 1.3005, acc: 47.97%.\n",
      "--- epoch: 5 ---\n",
      "training loss: 1.3220, acc: 47.97%, dev loss: 1.3274, acc: 52.03%.\n",
      "--- epoch: 6 ---\n",
      "training loss: 1.3595, acc: 47.30%, dev loss: 1.2568, acc: 54.73%.\n",
      "--- epoch: 7 ---\n",
      "training loss: 1.3112, acc: 48.65%, dev loss: 1.1797, acc: 51.35%.\n",
      "--- epoch: 8 ---\n",
      "training loss: 1.2035, acc: 54.05%, dev loss: 1.1289, acc: 57.43%.\n",
      "--- epoch: 9 ---\n",
      "training loss: 1.2370, acc: 51.86%, dev loss: 1.0695, acc: 56.76%.\n",
      "--- epoch: 10 ---\n",
      "training loss: 1.1895, acc: 50.84%, dev loss: 1.1178, acc: 58.78%.\n",
      "--- epoch: 11 ---\n",
      "training loss: 1.1510, acc: 54.39%, dev loss: 1.0927, acc: 60.81%.\n",
      "--- epoch: 12 ---\n",
      "training loss: 1.1123, acc: 54.73%, dev loss: 1.0552, acc: 63.51%.\n",
      "--- epoch: 13 ---\n",
      "training loss: 1.0935, acc: 58.61%, dev loss: 1.0013, acc: 62.84%.\n",
      "--- epoch: 14 ---\n",
      "training loss: 1.1288, acc: 56.93%, dev loss: 0.9886, acc: 61.49%.\n",
      "--- epoch: 15 ---\n",
      "training loss: 1.1142, acc: 57.94%, dev loss: 0.9376, acc: 66.22%.\n",
      "--- epoch: 16 ---\n",
      "training loss: 0.9718, acc: 64.19%, dev loss: 0.8316, acc: 68.92%.\n",
      "--- epoch: 17 ---\n",
      "training loss: 0.9384, acc: 63.18%, dev loss: 0.9211, acc: 62.16%.\n",
      "--- epoch: 18 ---\n",
      "training loss: 0.9658, acc: 61.99%, dev loss: 0.7760, acc: 69.59%.\n",
      "--- epoch: 19 ---\n",
      "training loss: 0.9074, acc: 66.39%, dev loss: 0.8877, acc: 68.24%.\n",
      "--- epoch: 20 ---\n",
      "training loss: 0.8593, acc: 69.09%, dev loss: 0.7508, acc: 66.89%.\n",
      "--- epoch: 21 ---\n",
      "training loss: 0.8878, acc: 65.88%, dev loss: 0.8537, acc: 70.27%.\n",
      "--- epoch: 22 ---\n",
      "training loss: 0.8954, acc: 64.19%, dev loss: 0.8768, acc: 62.84%.\n",
      "--- epoch: 23 ---\n",
      "training loss: 0.7973, acc: 72.64%, dev loss: 0.7436, acc: 71.62%.\n",
      "--- epoch: 24 ---\n",
      "training loss: 0.8123, acc: 70.78%, dev loss: 0.7057, acc: 70.95%.\n",
      "--- epoch: 25 ---\n",
      "training loss: 0.7445, acc: 72.64%, dev loss: 0.7727, acc: 65.54%.\n",
      "--- epoch: 26 ---\n",
      "training loss: 0.7556, acc: 69.93%, dev loss: 0.7504, acc: 69.59%.\n",
      "--- epoch: 27 ---\n",
      "training loss: 0.7371, acc: 72.80%, dev loss: 0.6806, acc: 72.30%.\n",
      "--- epoch: 28 ---\n",
      "training loss: 0.7592, acc: 71.45%, dev loss: 0.7124, acc: 70.27%.\n",
      "--- epoch: 29 ---\n",
      "training loss: 0.7268, acc: 74.16%, dev loss: 0.7275, acc: 70.95%.\n",
      "--- epoch: 30 ---\n",
      "training loss: 0.7751, acc: 72.47%, dev loss: 0.7066, acc: 72.30%.\n",
      "--- epoch: 31 ---\n",
      "training loss: 0.6665, acc: 76.18%, dev loss: 0.8307, acc: 64.86%.\n",
      "--- epoch: 32 ---\n",
      "training loss: 0.7339, acc: 72.47%, dev loss: 0.8110, acc: 67.57%.\n",
      "--- epoch: 33 ---\n",
      "training loss: 0.7447, acc: 72.64%, dev loss: 0.7349, acc: 74.32%.\n",
      "--- epoch: 34 ---\n",
      "training loss: 0.6366, acc: 76.86%, dev loss: 0.6515, acc: 72.30%.\n",
      "--- epoch: 35 ---\n",
      "training loss: 0.6591, acc: 75.68%, dev loss: 0.7988, acc: 68.24%.\n",
      "--- epoch: 36 ---\n",
      "training loss: 0.6892, acc: 73.99%, dev loss: 0.6612, acc: 73.65%.\n",
      "--- epoch: 37 ---\n",
      "training loss: 0.6654, acc: 75.17%, dev loss: 0.6695, acc: 73.65%.\n",
      "--- epoch: 38 ---\n",
      "training loss: 0.5827, acc: 78.89%, dev loss: 0.6152, acc: 75.00%.\n",
      "--- epoch: 39 ---\n",
      "training loss: 0.6699, acc: 74.66%, dev loss: 0.7187, acc: 71.62%.\n",
      "--- epoch: 40 ---\n",
      "training loss: 0.6446, acc: 76.52%, dev loss: 0.6672, acc: 70.95%.\n",
      "--- epoch: 41 ---\n",
      "training loss: 0.6643, acc: 76.86%, dev loss: 0.7144, acc: 72.30%.\n",
      "--- epoch: 42 ---\n",
      "training loss: 0.6394, acc: 76.01%, dev loss: 0.8374, acc: 66.89%.\n",
      "--- epoch: 43 ---\n",
      "training loss: 0.6797, acc: 75.68%, dev loss: 0.6933, acc: 74.32%.\n",
      "--- epoch: 44 ---\n",
      "training loss: 0.6419, acc: 76.35%, dev loss: 0.6685, acc: 73.65%.\n",
      "--- epoch: 45 ---\n",
      "training loss: 0.6280, acc: 75.84%, dev loss: 0.7089, acc: 70.95%.\n",
      "--- epoch: 46 ---\n",
      "training loss: 0.5922, acc: 78.04%, dev loss: 0.7116, acc: 72.97%.\n",
      "--- epoch: 47 ---\n",
      "training loss: 0.6465, acc: 77.36%, dev loss: 0.7378, acc: 72.30%.\n",
      "--- epoch: 48 ---\n",
      "training loss: 0.6459, acc: 75.17%, dev loss: 0.6327, acc: 72.97%.\n",
      "--- epoch: 49 ---\n",
      "training loss: 0.6679, acc: 78.21%, dev loss: 0.7089, acc: 70.95%.\n",
      "--- epoch: 50 ---\n",
      "training loss: 0.6018, acc: 79.22%, dev loss: 0.6677, acc: 72.30%.\n",
      "--- epoch: 51 ---\n",
      "training loss: 0.5488, acc: 81.42%, dev loss: 0.5670, acc: 77.70%.\n",
      "--- epoch: 52 ---\n",
      "training loss: 0.5589, acc: 80.07%, dev loss: 0.6599, acc: 73.65%.\n",
      "--- epoch: 53 ---\n",
      "training loss: 0.5508, acc: 80.57%, dev loss: 0.6342, acc: 75.00%.\n",
      "--- epoch: 54 ---\n",
      "training loss: 0.5446, acc: 80.07%, dev loss: 0.6527, acc: 75.68%.\n",
      "--- epoch: 55 ---\n",
      "training loss: 0.5838, acc: 79.90%, dev loss: 0.6229, acc: 79.73%.\n",
      "--- epoch: 56 ---\n",
      "training loss: 0.5722, acc: 80.41%, dev loss: 0.5960, acc: 75.00%.\n",
      "--- epoch: 57 ---\n",
      "training loss: 0.5676, acc: 81.59%, dev loss: 0.6402, acc: 72.30%.\n",
      "--- epoch: 58 ---\n",
      "training loss: 0.5785, acc: 79.56%, dev loss: 0.7532, acc: 70.27%.\n",
      "--- epoch: 59 ---\n",
      "training loss: 0.5912, acc: 79.05%, dev loss: 0.6145, acc: 75.68%.\n",
      "--- epoch: 60 ---\n",
      "training loss: 0.4887, acc: 83.28%, dev loss: 0.6528, acc: 74.32%.\n",
      "--- epoch: 61 ---\n",
      "training loss: 0.5132, acc: 81.42%, dev loss: 0.6065, acc: 75.68%.\n",
      "--- epoch: 62 ---\n",
      "training loss: 0.5319, acc: 82.09%, dev loss: 0.7308, acc: 70.27%.\n",
      "--- epoch: 63 ---\n",
      "training loss: 0.6188, acc: 77.53%, dev loss: 0.6686, acc: 71.62%.\n",
      "--- epoch: 64 ---\n",
      "training loss: 0.4741, acc: 84.46%, dev loss: 0.6184, acc: 74.32%.\n",
      "--- epoch: 65 ---\n",
      "training loss: 0.6823, acc: 77.36%, dev loss: 0.7773, acc: 71.62%.\n",
      "--- epoch: 66 ---\n",
      "training loss: 0.5724, acc: 80.07%, dev loss: 0.5934, acc: 75.68%.\n",
      "--- epoch: 67 ---\n",
      "training loss: 0.6084, acc: 78.38%, dev loss: 0.6499, acc: 74.32%.\n",
      "--- epoch: 68 ---\n",
      "training loss: 0.5111, acc: 81.76%, dev loss: 0.6521, acc: 73.65%.\n",
      "--- epoch: 69 ---\n",
      "training loss: 0.4663, acc: 82.43%, dev loss: 0.6081, acc: 76.35%.\n",
      "--- epoch: 70 ---\n",
      "training loss: 0.5323, acc: 79.39%, dev loss: 0.6253, acc: 75.68%.\n",
      "--- epoch: 71 ---\n",
      "training loss: 0.5030, acc: 82.26%, dev loss: 0.6367, acc: 72.97%.\n",
      "--- epoch: 72 ---\n",
      "training loss: 0.4823, acc: 84.12%, dev loss: 0.7149, acc: 75.00%.\n",
      "--- epoch: 73 ---\n",
      "training loss: 0.4853, acc: 82.77%, dev loss: 0.6133, acc: 75.00%.\n",
      "--- epoch: 74 ---\n",
      "training loss: 0.5006, acc: 81.76%, dev loss: 0.6664, acc: 74.32%.\n",
      "--- epoch: 75 ---\n",
      "training loss: 0.4420, acc: 83.95%, dev loss: 0.6568, acc: 68.92%.\n",
      "--- epoch: 76 ---\n",
      "training loss: 0.4958, acc: 81.93%, dev loss: 0.6764, acc: 75.68%.\n",
      "--- epoch: 77 ---\n",
      "training loss: 0.5410, acc: 80.41%, dev loss: 0.6321, acc: 72.97%.\n",
      "--- epoch: 78 ---\n",
      "training loss: 0.4988, acc: 82.77%, dev loss: 0.6664, acc: 75.68%.\n",
      "--- epoch: 79 ---\n",
      "training loss: 0.5073, acc: 81.93%, dev loss: 0.6680, acc: 74.32%.\n",
      "--- epoch: 80 ---\n",
      "training loss: 0.5063, acc: 83.11%, dev loss: 0.6064, acc: 75.00%.\n",
      "--- epoch: 81 ---\n",
      "training loss: 0.4485, acc: 82.94%, dev loss: 0.6381, acc: 73.65%.\n",
      "--- epoch: 82 ---\n",
      "training loss: 0.5426, acc: 82.43%, dev loss: 0.6316, acc: 77.70%.\n",
      "--- epoch: 83 ---\n",
      "training loss: 0.4327, acc: 84.12%, dev loss: 0.6563, acc: 75.00%.\n",
      "--- epoch: 84 ---\n",
      "training loss: 0.5133, acc: 82.60%, dev loss: 0.7434, acc: 72.30%.\n",
      "--- epoch: 85 ---\n",
      "training loss: 0.4857, acc: 83.11%, dev loss: 0.6747, acc: 75.00%.\n",
      "--- epoch: 86 ---\n",
      "training loss: 0.5110, acc: 80.74%, dev loss: 0.6378, acc: 74.32%.\n",
      "--- epoch: 87 ---\n",
      "training loss: 0.5904, acc: 80.91%, dev loss: 0.7213, acc: 72.30%.\n",
      "--- epoch: 88 ---\n",
      "training loss: 0.5010, acc: 82.77%, dev loss: 0.6654, acc: 75.68%.\n",
      "--- epoch: 89 ---\n",
      "training loss: 0.4838, acc: 83.11%, dev loss: 0.7563, acc: 70.27%.\n",
      "--- epoch: 90 ---\n",
      "training loss: 0.4542, acc: 84.97%, dev loss: 0.6727, acc: 70.27%.\n",
      "--- epoch: 91 ---\n",
      "training loss: 0.5029, acc: 82.77%, dev loss: 0.6846, acc: 75.00%.\n",
      "--- epoch: 92 ---\n",
      "training loss: 0.5081, acc: 81.42%, dev loss: 0.6664, acc: 74.32%.\n",
      "--- epoch: 93 ---\n",
      "training loss: 0.4737, acc: 84.12%, dev loss: 0.6626, acc: 72.97%.\n",
      "--- epoch: 94 ---\n",
      "training loss: 0.4498, acc: 84.63%, dev loss: 0.6586, acc: 72.97%.\n",
      "--- epoch: 95 ---\n",
      "training loss: 0.4689, acc: 83.78%, dev loss: 0.6295, acc: 79.05%.\n",
      "--- epoch: 96 ---\n",
      "training loss: 0.4460, acc: 84.63%, dev loss: 0.6573, acc: 75.00%.\n",
      "--- epoch: 97 ---\n",
      "training loss: 0.4556, acc: 82.94%, dev loss: 0.6588, acc: 73.65%.\n",
      "--- epoch: 98 ---\n",
      "training loss: 0.4743, acc: 84.29%, dev loss: 0.7801, acc: 70.27%.\n",
      "--- epoch: 99 ---\n",
      "training loss: 0.6055, acc: 78.55%, dev loss: 0.7767, acc: 68.24%.\n",
      "--- epoch: 100 ---\n",
      "training loss: 0.4921, acc: 83.28%, dev loss: 0.6533, acc: 71.62%.\n",
      "best dev acc: 0.7973\n",
      "===========fold:3==============\n",
      "--- epoch: 1 ---\n",
      "training loss: 1.7860, acc: 25.84%, dev loss: 1.6191, acc: 29.73%.\n",
      "--- epoch: 2 ---\n",
      "training loss: 1.5389, acc: 35.30%, dev loss: 1.3922, acc: 44.59%.\n",
      "--- epoch: 3 ---\n",
      "training loss: 1.3879, acc: 41.39%, dev loss: 1.2797, acc: 45.27%.\n",
      "--- epoch: 4 ---\n",
      "training loss: 1.3659, acc: 43.58%, dev loss: 1.1968, acc: 54.05%.\n",
      "--- epoch: 5 ---\n",
      "training loss: 1.3031, acc: 46.11%, dev loss: 1.1424, acc: 57.43%.\n",
      "--- epoch: 6 ---\n",
      "training loss: 1.3521, acc: 44.43%, dev loss: 1.3576, acc: 40.54%.\n",
      "--- epoch: 7 ---\n",
      "training loss: 1.3755, acc: 43.92%, dev loss: 1.1183, acc: 58.11%.\n",
      "--- epoch: 8 ---\n",
      "training loss: 1.2802, acc: 48.14%, dev loss: 1.2038, acc: 48.65%.\n",
      "--- epoch: 9 ---\n",
      "training loss: 1.2983, acc: 45.27%, dev loss: 1.2027, acc: 54.73%.\n",
      "--- epoch: 10 ---\n",
      "training loss: 1.2032, acc: 50.51%, dev loss: 1.1150, acc: 60.81%.\n",
      "--- epoch: 11 ---\n",
      "training loss: 1.2023, acc: 51.18%, dev loss: 1.1715, acc: 51.35%.\n",
      "--- epoch: 12 ---\n",
      "training loss: 1.1613, acc: 55.91%, dev loss: 0.9786, acc: 64.86%.\n",
      "--- epoch: 13 ---\n",
      "training loss: 1.1800, acc: 57.26%, dev loss: 1.0130, acc: 67.57%.\n",
      "--- epoch: 14 ---\n",
      "training loss: 1.1056, acc: 61.49%, dev loss: 1.1638, acc: 52.70%.\n",
      "--- epoch: 15 ---\n",
      "training loss: 1.1478, acc: 56.76%, dev loss: 1.0673, acc: 58.11%.\n",
      "--- epoch: 16 ---\n",
      "training loss: 1.0399, acc: 60.98%, dev loss: 0.8356, acc: 64.86%.\n",
      "--- epoch: 17 ---\n",
      "training loss: 1.1102, acc: 58.45%, dev loss: 0.9091, acc: 67.57%.\n",
      "--- epoch: 18 ---\n",
      "training loss: 1.0529, acc: 60.30%, dev loss: 0.9514, acc: 63.51%.\n",
      "--- epoch: 19 ---\n",
      "training loss: 1.1262, acc: 57.60%, dev loss: 1.0956, acc: 58.78%.\n",
      "--- epoch: 20 ---\n",
      "training loss: 1.0793, acc: 57.43%, dev loss: 0.8743, acc: 68.24%.\n",
      "--- epoch: 21 ---\n",
      "training loss: 0.9340, acc: 66.05%, dev loss: 0.8322, acc: 69.59%.\n",
      "--- epoch: 22 ---\n",
      "training loss: 0.9466, acc: 65.20%, dev loss: 0.9196, acc: 60.14%.\n",
      "--- epoch: 23 ---\n",
      "training loss: 0.9063, acc: 66.22%, dev loss: 0.8134, acc: 68.92%.\n",
      "--- epoch: 24 ---\n",
      "training loss: 1.0066, acc: 64.70%, dev loss: 0.9576, acc: 67.57%.\n",
      "--- epoch: 25 ---\n",
      "training loss: 0.9529, acc: 61.82%, dev loss: 0.8873, acc: 60.81%.\n",
      "--- epoch: 26 ---\n",
      "training loss: 0.9019, acc: 65.54%, dev loss: 0.9474, acc: 60.14%.\n",
      "--- epoch: 27 ---\n",
      "training loss: 0.9186, acc: 68.07%, dev loss: 0.9098, acc: 64.19%.\n",
      "--- epoch: 28 ---\n",
      "training loss: 0.9985, acc: 63.68%, dev loss: 0.9602, acc: 61.49%.\n",
      "--- epoch: 29 ---\n",
      "training loss: 0.8620, acc: 69.93%, dev loss: 0.8103, acc: 66.89%.\n",
      "--- epoch: 30 ---\n",
      "training loss: 0.8795, acc: 66.55%, dev loss: 0.9895, acc: 61.49%.\n",
      "--- epoch: 31 ---\n",
      "training loss: 0.8977, acc: 65.37%, dev loss: 0.8999, acc: 62.84%.\n",
      "--- epoch: 32 ---\n",
      "training loss: 0.9168, acc: 67.06%, dev loss: 0.9227, acc: 67.57%.\n",
      "--- epoch: 33 ---\n",
      "training loss: 0.9131, acc: 64.86%, dev loss: 0.9004, acc: 68.24%.\n",
      "--- epoch: 34 ---\n",
      "training loss: 0.8522, acc: 67.74%, dev loss: 0.8182, acc: 66.22%.\n",
      "--- epoch: 35 ---\n",
      "training loss: 0.8896, acc: 67.57%, dev loss: 0.7904, acc: 66.89%.\n",
      "--- epoch: 36 ---\n",
      "training loss: 0.8940, acc: 67.40%, dev loss: 0.8277, acc: 70.27%.\n",
      "--- epoch: 37 ---\n",
      "training loss: 0.8932, acc: 67.23%, dev loss: 0.8096, acc: 69.59%.\n",
      "--- epoch: 38 ---\n",
      "training loss: 0.8118, acc: 69.93%, dev loss: 0.7493, acc: 70.95%.\n",
      "--- epoch: 39 ---\n",
      "training loss: 0.7542, acc: 72.13%, dev loss: 0.7560, acc: 70.27%.\n",
      "--- epoch: 40 ---\n",
      "training loss: 0.8226, acc: 70.78%, dev loss: 0.8542, acc: 62.16%.\n",
      "--- epoch: 41 ---\n",
      "training loss: 0.8210, acc: 69.09%, dev loss: 0.8517, acc: 70.95%.\n",
      "--- epoch: 42 ---\n",
      "training loss: 0.7745, acc: 71.45%, dev loss: 0.8127, acc: 70.27%.\n",
      "--- epoch: 43 ---\n",
      "training loss: 0.8986, acc: 65.71%, dev loss: 0.8306, acc: 68.92%.\n",
      "--- epoch: 44 ---\n",
      "training loss: 0.7874, acc: 69.76%, dev loss: 0.7976, acc: 66.89%.\n",
      "--- epoch: 45 ---\n",
      "training loss: 0.8494, acc: 68.41%, dev loss: 0.7733, acc: 67.57%.\n",
      "--- epoch: 46 ---\n",
      "training loss: 0.7923, acc: 71.62%, dev loss: 0.8294, acc: 67.57%.\n",
      "--- epoch: 47 ---\n",
      "training loss: 0.7164, acc: 72.97%, dev loss: 0.7433, acc: 68.24%.\n",
      "--- epoch: 48 ---\n",
      "training loss: 0.7970, acc: 71.28%, dev loss: 0.8698, acc: 62.16%.\n",
      "--- epoch: 49 ---\n",
      "training loss: 0.7600, acc: 70.95%, dev loss: 0.7824, acc: 66.89%.\n",
      "--- epoch: 50 ---\n",
      "training loss: 0.7327, acc: 72.13%, dev loss: 0.7579, acc: 69.59%.\n",
      "--- epoch: 51 ---\n",
      "training loss: 0.6931, acc: 74.83%, dev loss: 0.7386, acc: 68.92%.\n",
      "--- epoch: 52 ---\n",
      "training loss: 0.7675, acc: 70.10%, dev loss: 0.7165, acc: 71.62%.\n",
      "--- epoch: 53 ---\n",
      "training loss: 0.6938, acc: 76.18%, dev loss: 0.8332, acc: 64.19%.\n",
      "--- epoch: 54 ---\n",
      "training loss: 0.7136, acc: 74.83%, dev loss: 0.8143, acc: 68.24%.\n",
      "--- epoch: 55 ---\n",
      "training loss: 0.7762, acc: 71.28%, dev loss: 0.8888, acc: 64.86%.\n",
      "--- epoch: 56 ---\n",
      "training loss: 0.7306, acc: 74.49%, dev loss: 0.7126, acc: 70.95%.\n",
      "--- epoch: 57 ---\n",
      "training loss: 0.7351, acc: 73.99%, dev loss: 0.7026, acc: 70.95%.\n",
      "--- epoch: 58 ---\n",
      "training loss: 0.6759, acc: 75.00%, dev loss: 0.7672, acc: 67.57%.\n",
      "--- epoch: 59 ---\n",
      "training loss: 0.7148, acc: 75.51%, dev loss: 0.7047, acc: 68.24%.\n",
      "--- epoch: 60 ---\n",
      "training loss: 0.5731, acc: 78.38%, dev loss: 0.6383, acc: 72.97%.\n",
      "--- epoch: 61 ---\n",
      "training loss: 0.7451, acc: 71.45%, dev loss: 0.6665, acc: 72.30%.\n",
      "--- epoch: 62 ---\n",
      "training loss: 0.6777, acc: 76.69%, dev loss: 0.7503, acc: 65.54%.\n",
      "--- epoch: 63 ---\n",
      "training loss: 0.6699, acc: 76.35%, dev loss: 0.7349, acc: 68.92%.\n",
      "--- epoch: 64 ---\n",
      "training loss: 0.6578, acc: 77.87%, dev loss: 0.7808, acc: 66.89%.\n",
      "--- epoch: 65 ---\n",
      "training loss: 0.6495, acc: 77.53%, dev loss: 0.7474, acc: 71.62%.\n",
      "--- epoch: 66 ---\n",
      "training loss: 0.6606, acc: 75.00%, dev loss: 0.6980, acc: 70.27%.\n",
      "--- epoch: 67 ---\n",
      "training loss: 0.5990, acc: 78.89%, dev loss: 1.2325, acc: 54.73%.\n",
      "--- epoch: 68 ---\n",
      "training loss: 0.6282, acc: 77.53%, dev loss: 0.6560, acc: 72.97%.\n",
      "--- epoch: 69 ---\n",
      "training loss: 0.5505, acc: 78.38%, dev loss: 0.7861, acc: 68.92%.\n",
      "--- epoch: 70 ---\n",
      "training loss: 0.6442, acc: 75.84%, dev loss: 0.6346, acc: 69.59%.\n",
      "--- epoch: 71 ---\n",
      "training loss: 0.5726, acc: 78.89%, dev loss: 0.7049, acc: 70.95%.\n",
      "--- epoch: 72 ---\n",
      "training loss: 0.5376, acc: 80.91%, dev loss: 0.6391, acc: 73.65%.\n",
      "--- epoch: 73 ---\n",
      "training loss: 0.5805, acc: 77.70%, dev loss: 0.8144, acc: 69.59%.\n",
      "--- epoch: 74 ---\n",
      "training loss: 0.7289, acc: 75.51%, dev loss: 0.7044, acc: 72.97%.\n",
      "--- epoch: 75 ---\n",
      "training loss: 0.6275, acc: 78.04%, dev loss: 0.7142, acc: 68.24%.\n",
      "--- epoch: 76 ---\n",
      "training loss: 0.5911, acc: 79.90%, dev loss: 0.6237, acc: 72.97%.\n",
      "--- epoch: 77 ---\n",
      "training loss: 0.6253, acc: 76.01%, dev loss: 0.6375, acc: 70.95%.\n",
      "--- epoch: 78 ---\n",
      "training loss: 0.6243, acc: 77.20%, dev loss: 0.6538, acc: 69.59%.\n",
      "--- epoch: 79 ---\n",
      "training loss: 0.6180, acc: 77.87%, dev loss: 0.6739, acc: 71.62%.\n",
      "--- epoch: 80 ---\n",
      "training loss: 0.6313, acc: 75.68%, dev loss: 0.6574, acc: 72.30%.\n",
      "--- epoch: 81 ---\n",
      "training loss: 0.5740, acc: 79.05%, dev loss: 0.6338, acc: 70.27%.\n",
      "--- epoch: 82 ---\n",
      "training loss: 0.6064, acc: 77.87%, dev loss: 0.6989, acc: 70.27%.\n",
      "--- epoch: 83 ---\n",
      "training loss: 0.5702, acc: 78.89%, dev loss: 0.6061, acc: 72.30%.\n",
      "--- epoch: 84 ---\n",
      "training loss: 0.5986, acc: 77.87%, dev loss: 0.6346, acc: 72.30%.\n",
      "--- epoch: 85 ---\n",
      "training loss: 0.5406, acc: 81.08%, dev loss: 0.6635, acc: 73.65%.\n",
      "--- epoch: 86 ---\n",
      "training loss: 0.5700, acc: 78.89%, dev loss: 0.7882, acc: 66.22%.\n",
      "--- epoch: 87 ---\n",
      "training loss: 0.5431, acc: 78.38%, dev loss: 0.6577, acc: 73.65%.\n",
      "--- epoch: 88 ---\n",
      "training loss: 0.5786, acc: 78.04%, dev loss: 0.6551, acc: 72.30%.\n",
      "--- epoch: 89 ---\n",
      "training loss: 0.5837, acc: 78.21%, dev loss: 0.5726, acc: 77.70%.\n",
      "--- epoch: 90 ---\n",
      "training loss: 0.5305, acc: 82.09%, dev loss: 0.5395, acc: 79.05%.\n",
      "--- epoch: 91 ---\n",
      "training loss: 0.5418, acc: 80.57%, dev loss: 0.5898, acc: 75.68%.\n",
      "--- epoch: 92 ---\n",
      "training loss: 0.5528, acc: 78.89%, dev loss: 0.6191, acc: 76.35%.\n",
      "--- epoch: 93 ---\n",
      "training loss: 0.5276, acc: 80.57%, dev loss: 0.6346, acc: 75.68%.\n",
      "--- epoch: 94 ---\n",
      "training loss: 0.4962, acc: 82.43%, dev loss: 0.5901, acc: 75.00%.\n",
      "--- epoch: 95 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23244\\1447150183.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mbest_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_data_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'C:\\\\Users\\zzzgry\\Desktop\\midspore_lab2\\model\\\\pytorch\\\\fold{fold+1}-model-'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%m-%d_%H.%M'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23244\\3792962928.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_data_loader, dev_data_loader)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdev_data_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0mpred_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0mdev_stat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\basework\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23244\\452547633.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\basework\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\basework\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\basework\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\basework\\lib\\site-packages\\torch\\nn\\modules\\pooling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m    140\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                             self.return_indices)\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\basework\\lib\\site-packages\\torch\\_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\basework\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[0mstride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     return torch.max_pool2d(\n\u001b[1;32m--> 488\u001b[1;33m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m max_pool2d = boolean_dispatch(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "K = 5\n",
    "skf = StratifiedKFold(n_splits=K, random_state=seed, shuffle=True)\n",
    "models = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(data, label)):\n",
    "    print(f'===========fold:{fold+1}==============')\n",
    "    train_data, train_label = data[train_idx], label[train_idx]\n",
    "    val_data, val_label = data[val_idx], label[val_idx]\n",
    "    train_set = FlowerDataset(train_data, train_label)\n",
    "    dev_set = FlowerDataset(val_data, val_label)\n",
    "\n",
    "    train_data_loader = DataLoader(train_set, 32, True)\n",
    "    dev_data_loader = DataLoader(dev_set, 32, False)\n",
    "\n",
    "    net = MyModel()\n",
    "    best_net = train(net, train_data_loader, dev_data_loader)\n",
    "    model_path = f'C:\\\\Users\\zzzgry\\Desktop\\midspore_lab2\\model\\\\pytorch\\\\fold{fold+1}-model-'+time.strftime('%m-%d_%H.%M', time.localtime())+'.pkl'\n",
    "    torch.save(best_net, model_path)\n",
    "    net.load_state_dict(torch.load(model_path))\n",
    "    models.append(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data_loader):\n",
    "    test_stat = Stat(training=False)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader:\n",
    "            data, labels = batch[0], batch[1]\n",
    "            pred_outputs = model(data)\n",
    "            test_stat.add(pred_outputs, labels, 0)\n",
    "    return test_stat.pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 3, 100, 100]), torch.Size([6]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_test = '../TestImages/'\n",
    "imgs=[]\n",
    "for im in glob.glob(path_test+'/*.jpg'):\n",
    "    img=cv2.imread(im)           \n",
    "    img=cv2.resize(img,(w,h)) \n",
    "    imgs.append(img)  \n",
    "imgs = np.asarray(imgs,np.float32)    \n",
    "\n",
    "test_data = imgs / 255\n",
    "test_label = [x for x in range(len(test_data))]\n",
    "\n",
    "test_data = torch.FloatTensor(test_data).permute(0, 3, 1, 2)\n",
    "test_label = torch.LongTensor(test_label)\n",
    "test_data.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 0, 5]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = FlowerDataset(test_data, test_label)\n",
    "test_data_loader = DataLoader(test_set, batch_size=len(test_data), shuffle=False)\n",
    "test(net, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "pred_labels = []\n",
    "for net in models:\n",
    "    pred_label = test(net, test_data_loader)\n",
    "    pred_labels.append(pred_label)\n",
    "pred_labels = np.array(pred_labels).T\n",
    "# 这样pred_labels的行向量为第i个样本的预测值\n",
    "final_pred = []\n",
    "for sample in pred_labels:\n",
    "    final_pred.append(np.argmax(np.bincount(sample)))\n",
    "\n",
    "correct = [1 if pred_label==idx else 0 for idx, pred_label in enumerate(final_pred)]\n",
    "print(f'acc = {sum(correct)/len(correct)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.972972972972973\n"
     ]
    }
   ],
   "source": [
    "# pred_labels = []\n",
    "# for net in models:\n",
    "#     pred_label = test(net, dev_data_loader)\n",
    "#     pred_labels.append(pred_label)\n",
    "# pred_labels = np.array(pred_labels).T\n",
    "# # 这样pred_labels的行向量为第i个样本的预测值\n",
    "# final_pred = []\n",
    "# for sample in pred_labels:\n",
    "#     final_pred.append(np.argmax(np.bincount(sample)))\n",
    "\n",
    "# correct = [1 if pred_label==val_label[idx] else 0 for idx, pred_label in enumerate(final_pred)]\n",
    "# print(f'acc = {sum(correct)/len(correct)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = MyModel()\n",
    "# best_net = train(net, train_data_loader, dev_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(best_net, model_path)\n",
    "# net.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, test_data_loader):\n",
    "#     test_stat = Stat(training=False)\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_data_loader:\n",
    "#             data, labels = batch[0], batch[1]\n",
    "#             pred_outputs = model(data)\n",
    "#             test_stat.add(pred_outputs, labels, 0)\n",
    "\n",
    "#     report = classification_report(\n",
    "#         test_stat.labels,\n",
    "#         test_stat.pred_labels,\n",
    "#         target_names=class_list,\n",
    "#         digits=4,\n",
    "#         zero_division=0,\n",
    "#     )\n",
    "#     print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_test = '../TestImages/'\n",
    "# imgs=[]\n",
    "# for im in glob.glob(path_test+'/*.jpg'):\n",
    "#     img=cv2.imread(im)           \n",
    "#     img=cv2.resize(img,(w,h)) \n",
    "#     imgs.append(img)  \n",
    "# imgs = np.asarray(imgs,np.float32)    \n",
    "\n",
    "# test_data = imgs / 255\n",
    "# test_label = [x for x in range(len(test_data))]\n",
    "\n",
    "# test_data = torch.FloatTensor(test_data).permute(0, 3, 1, 2)\n",
    "# test_label = torch.LongTensor(test_label)\n",
    "# test_data.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = FlowerDataset(test_data, test_label)\n",
    "# test_data_loader = DataLoader(test_set, batch_size=len(test_data), shuffle=False)\n",
    "# test(net, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = MyModel()\n",
    "# net.load_state_dict(torch.load(model_path))\n",
    "# test(net, test_data_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0a0aef9578cc1edfecec80d6a58c391f944f9223b5cad1c1ac9c6fdf88bcc20"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('basework')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
